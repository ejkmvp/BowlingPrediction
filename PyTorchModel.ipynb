{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83abf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4533b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49651224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that pulls data from file as requested\n",
    "\"\"\" This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.f.seek(idx * 27)\n",
    "        gameData = self.f.read(27)\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fe4d5-830a-4a7e-872e-5bcd450b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that loads all data into GPU memory\n",
    "\"\"\" #this works significantly better but we are hard limited by vram. maybe try to look into memory pinning more\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        if self.length > 1500000:\n",
    "            self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        # build entire array into memory\n",
    "        self.inputArray=[]\n",
    "        self.finalScoreArray=[]\n",
    "        for v in range(self.length):\n",
    "            gameData = self.f.read(27)\n",
    "            tempArray = []\n",
    "            for x in range(25):\n",
    "                tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "            self.finalScoreArray.append(gameData[-1] * 256 + gameData[-2])\n",
    "            self.inputArray.append([float(v) for v in tempArray][:120])\n",
    "            if not v % 10000:\n",
    "                print(f\"loaded {v} our of {self.length}\")\n",
    "        self.inputTensor = torch.tensor(self.inputArray).to(\"cuda\")\n",
    "        self.outputTensor = torch.tensor(self.finalScoreArray).to(\"cuda\")\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputTensor[idx], self.outputTensor[idx]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16999c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.relu_stack(x)\n",
    "        output = torch.nn.Sigmoid()(logits)\n",
    "        return output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(trainData[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4a879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    totalBatches = len(dataloader)\n",
    "    model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction + loss\n",
    "        prediction = model(x.cuda()).squeeze(1)\n",
    "        loss = loss_fn(prediction, y.cuda())\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            print(f\"loss: {loss.item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "            print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    model.eval() # need to look into what this does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x.cuda())\n",
    "            test_loss += loss_fn(pred, y.cuda()).item()\n",
    "    \n",
    "    print(f\"Average Loss: {test_loss/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0211bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed9adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True, pin_memory=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b48a178-f1e4-4c4d-9a3b-3a174f61980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 0 games\n",
      "finish 1000 games\n",
      "finish 2000 games\n",
      "finish 3000 games\n",
      "finish 4000 games\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trainData)):\n\u001b[0;32m      4\u001b[0m     game, fScore \u001b[38;5;241m=\u001b[39m trainData[x]\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frameNum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pinNum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m game[\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m*\u001b[39m frameNum \u001b[38;5;241m+\u001b[39m pinNum] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# verify that games loaded from the traindataset are valid\n",
    "badGameCounter = 0\n",
    "for x in range(len(trainData)):\n",
    "    game, fScore = trainData[x]\n",
    "    for frameNum in range(5):\n",
    "        for pinNum in range(10):\n",
    "            if game[20 * frameNum + pinNum] == float(0):\n",
    "                if(game[20 * frameNum + pinNum + 10] == float(1)):\n",
    "                    badGameCounter += 1\n",
    "                    #print(f\"problem with game number {x}\")\n",
    "                    #print(20 * frameNum + pinNum)\n",
    "                    #print(20 * frameNum + pinNum + 10)\n",
    "                    #assert(False)\n",
    "    if not x % 1000:\n",
    "        print(f\"finish {x} games\")\n",
    "print(badGameCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f61d6a3-544d-4b36-b1fb-e4e541bb20e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[11][0][95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74655ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "129769\n"
     ]
    }
   ],
   "source": [
    "# test to see if pinning is working\n",
    "for batch_ndx, sample in enumerate(trainDataLoader):\n",
    "    if batch_ndx > 5:\n",
    "        break\n",
    "    print(sample[1].is_pinned())\n",
    "    print(sample[1].is_cuda)\n",
    "print(len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8191c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "loss: 3927.55\tbatch num: 0/129769\n",
      "took 13.695598840713501 seconds\n",
      "loss: 246.49\tbatch num: 1000/129769\n",
      "took 8.736359119415283 seconds\n",
      "loss: 329.44\tbatch num: 2000/129769\n",
      "took 8.632001876831055 seconds\n",
      "loss: 263.71\tbatch num: 3000/129769\n",
      "took 8.826998472213745 seconds\n",
      "loss: 304.69\tbatch num: 4000/129769\n",
      "took 8.888002634048462 seconds\n",
      "loss: 300.23\tbatch num: 5000/129769\n",
      "took 8.473998069763184 seconds\n",
      "loss: 219.16\tbatch num: 6000/129769\n",
      "took 8.69479513168335 seconds\n",
      "loss: 285.26\tbatch num: 7000/129769\n",
      "took 8.692012310028076 seconds\n",
      "loss: 337.56\tbatch num: 8000/129769\n",
      "took 8.558001041412354 seconds\n",
      "loss: 398.71\tbatch num: 9000/129769\n",
      "took 8.61580753326416 seconds\n",
      "loss: 229.21\tbatch num: 10000/129769\n",
      "took 8.483516693115234 seconds\n",
      "loss: 270.11\tbatch num: 11000/129769\n",
      "took 8.588001012802124 seconds\n",
      "loss: 131.00\tbatch num: 12000/129769\n",
      "took 8.56793761253357 seconds\n",
      "loss: 228.11\tbatch num: 13000/129769\n",
      "took 8.635440826416016 seconds\n",
      "loss: 294.42\tbatch num: 14000/129769\n",
      "took 8.901002645492554 seconds\n",
      "loss: 197.97\tbatch num: 15000/129769\n",
      "took 8.730371713638306 seconds\n",
      "loss: 211.99\tbatch num: 16000/129769\n",
      "took 8.388899803161621 seconds\n",
      "loss: 223.78\tbatch num: 17000/129769\n",
      "took 8.477867603302002 seconds\n",
      "loss: 299.25\tbatch num: 18000/129769\n",
      "took 8.495699644088745 seconds\n",
      "loss: 176.79\tbatch num: 19000/129769\n",
      "took 8.161109924316406 seconds\n",
      "loss: 210.37\tbatch num: 20000/129769\n",
      "took 8.385999202728271 seconds\n",
      "loss: 252.29\tbatch num: 21000/129769\n",
      "took 8.184876441955566 seconds\n",
      "loss: 287.84\tbatch num: 22000/129769\n",
      "took 8.275814294815063 seconds\n",
      "loss: 258.52\tbatch num: 23000/129769\n",
      "took 8.318899393081665 seconds\n",
      "loss: 237.99\tbatch num: 24000/129769\n",
      "took 8.656594514846802 seconds\n",
      "loss: 188.70\tbatch num: 25000/129769\n",
      "took 8.725139617919922 seconds\n",
      "loss: 265.65\tbatch num: 26000/129769\n",
      "took 8.607001781463623 seconds\n",
      "loss: 229.52\tbatch num: 27000/129769\n",
      "took 8.364797592163086 seconds\n",
      "loss: 194.80\tbatch num: 28000/129769\n",
      "took 8.498998880386353 seconds\n",
      "loss: 356.47\tbatch num: 29000/129769\n",
      "took 8.667001008987427 seconds\n",
      "loss: 256.13\tbatch num: 30000/129769\n",
      "took 8.776708364486694 seconds\n",
      "loss: 207.19\tbatch num: 31000/129769\n",
      "took 8.65200161933899 seconds\n",
      "loss: 185.36\tbatch num: 32000/129769\n",
      "took 8.548999309539795 seconds\n",
      "loss: 306.57\tbatch num: 33000/129769\n",
      "took 8.354588747024536 seconds\n",
      "loss: 183.94\tbatch num: 34000/129769\n",
      "took 8.511965036392212 seconds\n",
      "loss: 285.31\tbatch num: 35000/129769\n",
      "took 8.468883752822876 seconds\n",
      "loss: 258.47\tbatch num: 36000/129769\n",
      "took 8.362000226974487 seconds\n",
      "loss: 230.56\tbatch num: 37000/129769\n",
      "took 7.795999526977539 seconds\n",
      "loss: 223.09\tbatch num: 38000/129769\n",
      "took 8.092000484466553 seconds\n",
      "loss: 226.81\tbatch num: 39000/129769\n",
      "took 7.9529993534088135 seconds\n",
      "loss: 214.95\tbatch num: 40000/129769\n",
      "took 8.284999370574951 seconds\n",
      "loss: 297.47\tbatch num: 41000/129769\n",
      "took 7.924000263214111 seconds\n",
      "loss: 222.84\tbatch num: 42000/129769\n",
      "took 7.826000213623047 seconds\n",
      "loss: 204.57\tbatch num: 43000/129769\n",
      "took 8.127001523971558 seconds\n",
      "loss: 260.40\tbatch num: 44000/129769\n",
      "took 7.7469987869262695 seconds\n",
      "loss: 218.19\tbatch num: 45000/129769\n",
      "took 7.786999940872192 seconds\n",
      "loss: 164.40\tbatch num: 46000/129769\n",
      "took 8.103999376296997 seconds\n",
      "loss: 158.96\tbatch num: 47000/129769\n",
      "took 7.889002323150635 seconds\n",
      "loss: 242.39\tbatch num: 48000/129769\n",
      "took 7.964958667755127 seconds\n",
      "loss: 246.35\tbatch num: 49000/129769\n",
      "took 7.968008756637573 seconds\n",
      "loss: 228.17\tbatch num: 50000/129769\n",
      "took 7.910998582839966 seconds\n",
      "loss: 139.05\tbatch num: 51000/129769\n",
      "took 7.838000297546387 seconds\n",
      "loss: 180.79\tbatch num: 52000/129769\n",
      "took 7.784000396728516 seconds\n",
      "loss: 207.64\tbatch num: 53000/129769\n",
      "took 7.813000917434692 seconds\n",
      "loss: 275.56\tbatch num: 54000/129769\n",
      "took 7.792998790740967 seconds\n",
      "loss: 231.02\tbatch num: 55000/129769\n",
      "took 7.776000022888184 seconds\n",
      "loss: 292.03\tbatch num: 56000/129769\n",
      "took 7.875999689102173 seconds\n",
      "loss: 247.26\tbatch num: 57000/129769\n",
      "took 7.725000858306885 seconds\n",
      "loss: 253.32\tbatch num: 58000/129769\n",
      "took 7.740004539489746 seconds\n",
      "loss: 183.52\tbatch num: 59000/129769\n",
      "took 7.790998220443726 seconds\n",
      "loss: 264.09\tbatch num: 60000/129769\n",
      "took 7.819508075714111 seconds\n",
      "loss: 197.89\tbatch num: 61000/129769\n",
      "took 7.795447111129761 seconds\n",
      "loss: 233.84\tbatch num: 62000/129769\n",
      "took 7.737996816635132 seconds\n",
      "loss: 265.17\tbatch num: 63000/129769\n",
      "took 7.765000104904175 seconds\n",
      "loss: 328.09\tbatch num: 64000/129769\n",
      "took 7.790517091751099 seconds\n",
      "loss: 226.44\tbatch num: 65000/129769\n",
      "took 7.808000564575195 seconds\n",
      "loss: 208.39\tbatch num: 66000/129769\n",
      "took 7.797019720077515 seconds\n",
      "loss: 270.51\tbatch num: 67000/129769\n",
      "took 7.764002561569214 seconds\n",
      "loss: 160.28\tbatch num: 68000/129769\n",
      "took 7.8410255908966064 seconds\n",
      "loss: 172.50\tbatch num: 69000/129769\n",
      "took 7.800999879837036 seconds\n",
      "loss: 210.58\tbatch num: 70000/129769\n",
      "took 7.856000900268555 seconds\n",
      "loss: 260.69\tbatch num: 71000/129769\n",
      "took 7.697019815444946 seconds\n",
      "loss: 144.77\tbatch num: 72000/129769\n",
      "took 7.826000213623047 seconds\n",
      "loss: 216.34\tbatch num: 73000/129769\n",
      "took 7.784001350402832 seconds\n",
      "loss: 139.20\tbatch num: 74000/129769\n",
      "took 8.006999015808105 seconds\n",
      "loss: 195.91\tbatch num: 75000/129769\n",
      "took 7.737999200820923 seconds\n",
      "loss: 155.94\tbatch num: 76000/129769\n",
      "took 7.709999322891235 seconds\n",
      "loss: 207.07\tbatch num: 77000/129769\n",
      "took 7.671001672744751 seconds\n",
      "loss: 192.43\tbatch num: 78000/129769\n",
      "took 7.898999214172363 seconds\n",
      "loss: 209.55\tbatch num: 79000/129769\n",
      "took 7.697010278701782 seconds\n",
      "loss: 163.44\tbatch num: 80000/129769\n",
      "took 7.714001178741455 seconds\n",
      "loss: 242.96\tbatch num: 81000/129769\n",
      "took 7.738999128341675 seconds\n",
      "loss: 274.17\tbatch num: 82000/129769\n",
      "took 7.996999502182007 seconds\n",
      "loss: 241.39\tbatch num: 83000/129769\n",
      "took 7.815999746322632 seconds\n",
      "loss: 163.59\tbatch num: 84000/129769\n",
      "took 7.796000242233276 seconds\n",
      "loss: 171.93\tbatch num: 85000/129769\n",
      "took 7.842000246047974 seconds\n",
      "loss: 209.35\tbatch num: 86000/129769\n",
      "took 8.126001119613647 seconds\n",
      "loss: 218.33\tbatch num: 87000/129769\n",
      "took 7.963740110397339 seconds\n",
      "loss: 202.12\tbatch num: 88000/129769\n",
      "took 7.965001106262207 seconds\n",
      "loss: 207.37\tbatch num: 89000/129769\n",
      "took 8.208997964859009 seconds\n",
      "loss: 204.91\tbatch num: 90000/129769\n",
      "took 7.825999975204468 seconds\n",
      "loss: 188.67\tbatch num: 91000/129769\n",
      "took 7.959160089492798 seconds\n",
      "loss: 218.12\tbatch num: 92000/129769\n",
      "took 7.91400146484375 seconds\n",
      "loss: 167.74\tbatch num: 93000/129769\n",
      "took 8.016000032424927 seconds\n",
      "loss: 269.83\tbatch num: 94000/129769\n",
      "took 7.807997703552246 seconds\n",
      "loss: 237.82\tbatch num: 95000/129769\n",
      "took 7.76200008392334 seconds\n",
      "loss: 242.84\tbatch num: 96000/129769\n",
      "took 7.76099967956543 seconds\n",
      "loss: 162.05\tbatch num: 97000/129769\n",
      "took 7.811001539230347 seconds\n",
      "loss: 215.38\tbatch num: 98000/129769\n",
      "took 7.760998964309692 seconds\n",
      "loss: 223.17\tbatch num: 99000/129769\n",
      "took 7.878000020980835 seconds\n",
      "loss: 213.50\tbatch num: 100000/129769\n",
      "took 7.811002492904663 seconds\n",
      "loss: 185.34\tbatch num: 101000/129769\n",
      "took 7.908998012542725 seconds\n",
      "loss: 243.06\tbatch num: 102000/129769\n",
      "took 7.958031415939331 seconds\n",
      "loss: 231.17\tbatch num: 103000/129769\n",
      "took 8.00299882888794 seconds\n",
      "loss: 273.38\tbatch num: 104000/129769\n",
      "took 8.415998458862305 seconds\n",
      "loss: 318.69\tbatch num: 105000/129769\n",
      "took 8.042011260986328 seconds\n",
      "loss: 233.45\tbatch num: 106000/129769\n",
      "took 7.9100000858306885 seconds\n",
      "loss: 319.84\tbatch num: 107000/129769\n",
      "took 7.680999994277954 seconds\n",
      "loss: 234.77\tbatch num: 108000/129769\n",
      "took 7.699033737182617 seconds\n",
      "loss: 145.70\tbatch num: 109000/129769\n",
      "took 7.7380006313323975 seconds\n",
      "loss: 228.63\tbatch num: 110000/129769\n",
      "took 7.701999664306641 seconds\n",
      "loss: 203.86\tbatch num: 111000/129769\n",
      "took 7.71399998664856 seconds\n",
      "loss: 312.69\tbatch num: 112000/129769\n",
      "took 7.681000471115112 seconds\n",
      "loss: 191.63\tbatch num: 113000/129769\n",
      "took 7.789000988006592 seconds\n",
      "loss: 294.11\tbatch num: 114000/129769\n",
      "took 8.315998077392578 seconds\n",
      "loss: 229.99\tbatch num: 115000/129769\n",
      "took 8.148000955581665 seconds\n",
      "loss: 271.48\tbatch num: 116000/129769\n",
      "took 8.202999114990234 seconds\n",
      "loss: 250.54\tbatch num: 117000/129769\n",
      "took 8.044394254684448 seconds\n",
      "loss: 197.99\tbatch num: 118000/129769\n",
      "took 7.788999319076538 seconds\n",
      "loss: 250.11\tbatch num: 119000/129769\n",
      "took 7.797000885009766 seconds\n",
      "loss: 324.09\tbatch num: 120000/129769\n",
      "took 7.868001699447632 seconds\n",
      "loss: 223.68\tbatch num: 121000/129769\n",
      "took 7.916997671127319 seconds\n",
      "loss: 252.51\tbatch num: 122000/129769\n",
      "took 7.809000015258789 seconds\n",
      "loss: 160.47\tbatch num: 123000/129769\n",
      "took 7.8580002784729 seconds\n",
      "loss: 198.07\tbatch num: 124000/129769\n",
      "took 8.167001008987427 seconds\n",
      "loss: 250.22\tbatch num: 125000/129769\n",
      "took 8.190860033035278 seconds\n",
      "loss: 283.76\tbatch num: 126000/129769\n",
      "took 8.385118961334229 seconds\n",
      "loss: 238.25\tbatch num: 127000/129769\n",
      "took 8.023999214172363 seconds\n",
      "loss: 232.04\tbatch num: 128000/129769\n",
      "took 8.0169997215271 seconds\n",
      "loss: 299.53\tbatch num: 129000/129769\n",
      "took 8.252003192901611 seconds\n",
      "Epoch 0 took 1062.123557806015 seconds\n",
      "Average Loss: 4857.547286987305\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 244.36\tbatch num: 0/129769\n",
      "took 0.44499850273132324 seconds\n",
      "loss: 216.05\tbatch num: 1000/129769\n",
      "took 7.765001535415649 seconds\n",
      "loss: 246.71\tbatch num: 2000/129769\n",
      "took 7.727999687194824 seconds\n",
      "loss: 256.66\tbatch num: 3000/129769\n",
      "took 7.728999853134155 seconds\n",
      "loss: 165.94\tbatch num: 4000/129769\n",
      "took 7.821998834609985 seconds\n",
      "loss: 216.39\tbatch num: 5000/129769\n",
      "took 7.745001792907715 seconds\n",
      "loss: 221.65\tbatch num: 6000/129769\n",
      "took 7.737999677658081 seconds\n",
      "loss: 171.59\tbatch num: 7000/129769\n",
      "took 7.721999883651733 seconds\n",
      "loss: 274.66\tbatch num: 8000/129769\n",
      "took 7.828999757766724 seconds\n",
      "loss: 230.62\tbatch num: 9000/129769\n",
      "took 7.75200343132019 seconds\n",
      "loss: 234.74\tbatch num: 10000/129769\n",
      "took 7.778996706008911 seconds\n",
      "loss: 162.82\tbatch num: 11000/129769\n",
      "took 7.747998952865601 seconds\n",
      "loss: 138.32\tbatch num: 12000/129769\n",
      "took 7.6970014572143555 seconds\n",
      "loss: 308.49\tbatch num: 13000/129769\n",
      "took 7.79699969291687 seconds\n",
      "loss: 221.13\tbatch num: 14000/129769\n",
      "took 7.686999559402466 seconds\n",
      "loss: 132.24\tbatch num: 15000/129769\n",
      "took 7.7190001010894775 seconds\n",
      "loss: 191.24\tbatch num: 16000/129769\n",
      "took 7.775000810623169 seconds\n",
      "loss: 205.06\tbatch num: 17000/129769\n",
      "took 7.708717584609985 seconds\n",
      "loss: 181.28\tbatch num: 18000/129769\n",
      "took 7.90300989151001 seconds\n",
      "loss: 195.90\tbatch num: 19000/129769\n",
      "took 7.965999126434326 seconds\n",
      "loss: 219.32\tbatch num: 20000/129769\n",
      "took 7.707000017166138 seconds\n",
      "loss: 182.69\tbatch num: 21000/129769\n",
      "took 7.688107490539551 seconds\n",
      "loss: 350.29\tbatch num: 22000/129769\n",
      "took 7.687000036239624 seconds\n",
      "loss: 212.60\tbatch num: 23000/129769\n",
      "took 7.7390007972717285 seconds\n",
      "loss: 173.22\tbatch num: 24000/129769\n",
      "took 7.760998964309692 seconds\n",
      "loss: 203.76\tbatch num: 25000/129769\n",
      "took 7.703000068664551 seconds\n",
      "loss: 300.41\tbatch num: 26000/129769\n",
      "took 7.988460540771484 seconds\n",
      "loss: 242.66\tbatch num: 27000/129769\n",
      "took 7.782996654510498 seconds\n",
      "loss: 235.26\tbatch num: 28000/129769\n",
      "took 7.892000675201416 seconds\n",
      "loss: 288.77\tbatch num: 29000/129769\n",
      "took 7.8670032024383545 seconds\n",
      "loss: 229.34\tbatch num: 30000/129769\n",
      "took 7.657997369766235 seconds\n",
      "loss: 242.21\tbatch num: 31000/129769\n",
      "took 7.735803604125977 seconds\n",
      "loss: 234.09\tbatch num: 32000/129769\n",
      "took 7.7629992961883545 seconds\n",
      "loss: 190.03\tbatch num: 33000/129769\n",
      "took 7.893983364105225 seconds\n",
      "loss: 250.06\tbatch num: 34000/129769\n",
      "took 7.681999444961548 seconds\n",
      "loss: 184.51\tbatch num: 35000/129769\n",
      "took 7.683000087738037 seconds\n",
      "loss: 192.83\tbatch num: 36000/129769\n",
      "took 7.684999942779541 seconds\n",
      "loss: 271.21\tbatch num: 37000/129769\n",
      "took 7.653001308441162 seconds\n",
      "loss: 215.45\tbatch num: 38000/129769\n",
      "took 7.648999214172363 seconds\n",
      "loss: 232.86\tbatch num: 39000/129769\n",
      "took 8.012999773025513 seconds\n",
      "loss: 255.83\tbatch num: 40000/129769\n",
      "took 7.672000169754028 seconds\n",
      "loss: 228.78\tbatch num: 41000/129769\n",
      "took 7.703644514083862 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDataLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstartTime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m test_loop(testDataLoader, model)\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# need to look into what this exactly does\u001b[39;00m\n\u001b[0;32m      5\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batchNum, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# zero out gradients\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# prediction + loss\u001b[39;00m\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mX:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TestModel().cuda()\n",
    "loss_fn = torch.nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, model)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"testmodel.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8ef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
