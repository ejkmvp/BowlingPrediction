{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83abf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4533b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49651224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that pulls data from file as requested\n",
    "\"\"\" This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.f.seek(idx * 27)\n",
    "        gameData = self.f.read(27)\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446fe4d5-830a-4a7e-872e-5bcd450b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that loads all data into GPU memory\n",
    "#this works significantly better but we are hard limited by vram. maybe try to look into memory pinning more\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        self.f.seek(0)\n",
    "        # build entire array into memory\n",
    "        self.inputArray=[]\n",
    "        self.finalScoreArray=[]\n",
    "        for v in range(self.length):\n",
    "            gameData = self.f.read(27)\n",
    "            tempArray = []\n",
    "            for x in range(25):\n",
    "                tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "            self.finalScoreArray.append(gameData[-1] * 256 + gameData[-2])\n",
    "            self.inputArray.append([float(v) for v in tempArray][:120])\n",
    "            if not v % 10000:\n",
    "                print(f\"loaded {v} our of {self.length}\")\n",
    "        self.inputTensor = torch.tensor(self.inputArray).to(\"cuda\")\n",
    "        self.outputTensor = torch.tensor(self.finalScoreArray).to(\"cuda\")\n",
    "        self.inputArray = []\n",
    "        self.finalScoreArray = []\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputTensor[idx], self.outputTensor[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16999c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.relu_stack(x)\n",
    "        output = torch.nn.Sigmoid()(logits)\n",
    "        return output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    totalBatches = len(dataloader)\n",
    "    model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction + loss\n",
    "        prediction = model(x).squeeze(1)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            print(f\"loss: {loss.item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "            print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval() # need to look into what this does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    print(f\"Average Loss: {test_loss/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0211bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed9adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 0 our of 1122352\n",
      "loaded 10000 our of 1122352\n",
      "loaded 20000 our of 1122352\n",
      "loaded 30000 our of 1122352\n",
      "loaded 40000 our of 1122352\n",
      "loaded 50000 our of 1122352\n",
      "loaded 60000 our of 1122352\n",
      "loaded 70000 our of 1122352\n",
      "loaded 80000 our of 1122352\n",
      "loaded 90000 our of 1122352\n",
      "loaded 100000 our of 1122352\n",
      "loaded 110000 our of 1122352\n",
      "loaded 120000 our of 1122352\n",
      "loaded 130000 our of 1122352\n",
      "loaded 140000 our of 1122352\n",
      "loaded 150000 our of 1122352\n",
      "loaded 160000 our of 1122352\n",
      "loaded 170000 our of 1122352\n",
      "loaded 180000 our of 1122352\n",
      "loaded 190000 our of 1122352\n",
      "loaded 200000 our of 1122352\n",
      "loaded 210000 our of 1122352\n",
      "loaded 220000 our of 1122352\n",
      "loaded 230000 our of 1122352\n",
      "loaded 240000 our of 1122352\n",
      "loaded 250000 our of 1122352\n",
      "loaded 260000 our of 1122352\n",
      "loaded 270000 our of 1122352\n",
      "loaded 280000 our of 1122352\n",
      "loaded 290000 our of 1122352\n",
      "loaded 300000 our of 1122352\n",
      "loaded 310000 our of 1122352\n",
      "loaded 320000 our of 1122352\n",
      "loaded 330000 our of 1122352\n",
      "loaded 340000 our of 1122352\n",
      "loaded 350000 our of 1122352\n",
      "loaded 360000 our of 1122352\n",
      "loaded 370000 our of 1122352\n",
      "loaded 380000 our of 1122352\n",
      "loaded 390000 our of 1122352\n",
      "loaded 400000 our of 1122352\n",
      "loaded 410000 our of 1122352\n",
      "loaded 420000 our of 1122352\n",
      "loaded 430000 our of 1122352\n",
      "loaded 440000 our of 1122352\n",
      "loaded 450000 our of 1122352\n",
      "loaded 460000 our of 1122352\n",
      "loaded 470000 our of 1122352\n",
      "loaded 480000 our of 1122352\n",
      "loaded 490000 our of 1122352\n",
      "loaded 500000 our of 1122352\n",
      "loaded 510000 our of 1122352\n",
      "loaded 520000 our of 1122352\n",
      "loaded 530000 our of 1122352\n",
      "loaded 540000 our of 1122352\n",
      "loaded 550000 our of 1122352\n",
      "loaded 560000 our of 1122352\n",
      "loaded 570000 our of 1122352\n",
      "loaded 580000 our of 1122352\n",
      "loaded 590000 our of 1122352\n",
      "loaded 600000 our of 1122352\n",
      "loaded 610000 our of 1122352\n",
      "loaded 620000 our of 1122352\n",
      "loaded 630000 our of 1122352\n",
      "loaded 640000 our of 1122352\n",
      "loaded 650000 our of 1122352\n",
      "loaded 660000 our of 1122352\n",
      "loaded 670000 our of 1122352\n",
      "loaded 680000 our of 1122352\n",
      "loaded 690000 our of 1122352\n",
      "loaded 700000 our of 1122352\n",
      "loaded 710000 our of 1122352\n",
      "loaded 720000 our of 1122352\n",
      "loaded 730000 our of 1122352\n",
      "loaded 740000 our of 1122352\n",
      "loaded 750000 our of 1122352\n",
      "loaded 760000 our of 1122352\n",
      "loaded 770000 our of 1122352\n",
      "loaded 780000 our of 1122352\n",
      "loaded 790000 our of 1122352\n",
      "loaded 800000 our of 1122352\n",
      "loaded 810000 our of 1122352\n",
      "loaded 820000 our of 1122352\n",
      "loaded 830000 our of 1122352\n",
      "loaded 840000 our of 1122352\n",
      "loaded 850000 our of 1122352\n",
      "loaded 860000 our of 1122352\n",
      "loaded 870000 our of 1122352\n",
      "loaded 880000 our of 1122352\n",
      "loaded 890000 our of 1122352\n",
      "loaded 900000 our of 1122352\n",
      "loaded 910000 our of 1122352\n",
      "loaded 920000 our of 1122352\n",
      "loaded 930000 our of 1122352\n",
      "loaded 940000 our of 1122352\n",
      "loaded 950000 our of 1122352\n",
      "loaded 960000 our of 1122352\n",
      "loaded 970000 our of 1122352\n",
      "loaded 980000 our of 1122352\n",
      "loaded 990000 our of 1122352\n",
      "loaded 1000000 our of 1122352\n",
      "loaded 1010000 our of 1122352\n",
      "loaded 1020000 our of 1122352\n",
      "loaded 1030000 our of 1122352\n",
      "loaded 1040000 our of 1122352\n",
      "loaded 1050000 our of 1122352\n",
      "loaded 1060000 our of 1122352\n",
      "loaded 1070000 our of 1122352\n",
      "loaded 1080000 our of 1122352\n",
      "loaded 1090000 our of 1122352\n",
      "loaded 1100000 our of 1122352\n",
      "loaded 1110000 our of 1122352\n",
      "loaded 1120000 our of 1122352\n",
      "loaded 0 our of 1000\n"
     ]
    }
   ],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74655ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test to see if pinning is working\n",
    "for batch_ndx, sample in enumerate(trainDataLoader):\n",
    "    if batch_ndx > 5:\n",
    "        break\n",
    "    print(sample[1].is_pinned())\n",
    "    print(sample[1].is_cuda)\n",
    "print(len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8191c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "loss: 35.55\tbatch num: 0/35074\n",
      "took 6.389125347137451 seconds\n",
      "loss: 15.57\tbatch num: 1000/35074\n",
      "took 3.5994176864624023 seconds\n",
      "loss: 11.47\tbatch num: 2000/35074\n",
      "took 3.5137290954589844 seconds\n",
      "loss: 11.63\tbatch num: 3000/35074\n",
      "took 3.384000778198242 seconds\n",
      "loss: 12.89\tbatch num: 4000/35074\n",
      "took 3.6650032997131348 seconds\n",
      "loss: 12.56\tbatch num: 5000/35074\n",
      "took 3.7047336101531982 seconds\n",
      "loss: 12.33\tbatch num: 6000/35074\n",
      "took 3.2140188217163086 seconds\n",
      "loss: 13.58\tbatch num: 7000/35074\n",
      "took 3.3389999866485596 seconds\n",
      "loss: 8.79\tbatch num: 8000/35074\n",
      "took 3.441030502319336 seconds\n",
      "loss: 12.13\tbatch num: 9000/35074\n",
      "took 3.349972724914551 seconds\n",
      "loss: 11.05\tbatch num: 10000/35074\n",
      "took 3.3950345516204834 seconds\n",
      "loss: 11.71\tbatch num: 11000/35074\n",
      "took 3.369993209838867 seconds\n",
      "loss: 13.85\tbatch num: 12000/35074\n",
      "took 3.4109981060028076 seconds\n",
      "loss: 10.49\tbatch num: 13000/35074\n",
      "took 3.364001512527466 seconds\n",
      "loss: 14.70\tbatch num: 14000/35074\n",
      "took 3.10296893119812 seconds\n",
      "loss: 15.99\tbatch num: 15000/35074\n",
      "took 2.3780033588409424 seconds\n",
      "loss: 15.99\tbatch num: 16000/35074\n",
      "took 2.4030303955078125 seconds\n",
      "loss: 8.29\tbatch num: 17000/35074\n",
      "took 2.44596791267395 seconds\n",
      "loss: 12.42\tbatch num: 18000/35074\n",
      "took 2.4849984645843506 seconds\n",
      "loss: 10.39\tbatch num: 19000/35074\n",
      "took 2.4830026626586914 seconds\n",
      "loss: 10.45\tbatch num: 20000/35074\n",
      "took 2.480996608734131 seconds\n",
      "loss: 13.52\tbatch num: 21000/35074\n",
      "took 2.3580000400543213 seconds\n",
      "loss: 13.21\tbatch num: 22000/35074\n",
      "took 2.3320014476776123 seconds\n",
      "loss: 12.04\tbatch num: 23000/35074\n",
      "took 2.329998731613159 seconds\n",
      "loss: 9.97\tbatch num: 24000/35074\n",
      "took 2.5470004081726074 seconds\n",
      "loss: 11.04\tbatch num: 25000/35074\n",
      "took 2.3718578815460205 seconds\n",
      "loss: 12.76\tbatch num: 26000/35074\n",
      "took 2.722085475921631 seconds\n",
      "loss: 13.19\tbatch num: 27000/35074\n",
      "took 2.4186806678771973 seconds\n",
      "loss: 14.58\tbatch num: 28000/35074\n",
      "took 2.520005941390991 seconds\n",
      "loss: 11.00\tbatch num: 29000/35074\n",
      "took 2.4869954586029053 seconds\n",
      "loss: 14.87\tbatch num: 30000/35074\n",
      "took 2.208998441696167 seconds\n",
      "loss: 14.69\tbatch num: 31000/35074\n",
      "took 2.1820003986358643 seconds\n",
      "loss: 17.02\tbatch num: 32000/35074\n",
      "took 2.160998821258545 seconds\n",
      "loss: 13.61\tbatch num: 33000/35074\n",
      "took 2.1680006980895996 seconds\n",
      "loss: 10.56\tbatch num: 34000/35074\n",
      "took 2.1760005950927734 seconds\n",
      "loss: 14.52\tbatch num: 35000/35074\n",
      "took 2.176999568939209 seconds\n",
      "Epoch 0 took 104.30865240097046 seconds\n",
      "Average Loss: 30.142254114151\n",
      "Starting epoch 1\n",
      "loss: 12.96\tbatch num: 0/35074\n",
      "took 0.05699872970581055 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.17\tbatch num: 1000/35074\n",
      "took 2.211000919342041 seconds\n",
      "loss: 14.45\tbatch num: 2000/35074\n",
      "took 2.174999952316284 seconds\n",
      "loss: 14.28\tbatch num: 3000/35074\n",
      "took 2.1590001583099365 seconds\n",
      "loss: 12.13\tbatch num: 4000/35074\n",
      "took 2.177999496459961 seconds\n",
      "loss: 14.13\tbatch num: 5000/35074\n",
      "took 2.188000440597534 seconds\n",
      "loss: 11.84\tbatch num: 6000/35074\n",
      "took 2.3500001430511475 seconds\n",
      "loss: 13.67\tbatch num: 7000/35074\n",
      "took 2.4529998302459717 seconds\n",
      "loss: 11.66\tbatch num: 8000/35074\n",
      "took 2.4800004959106445 seconds\n",
      "loss: 10.06\tbatch num: 9000/35074\n",
      "took 2.4589996337890625 seconds\n",
      "loss: 10.15\tbatch num: 10000/35074\n",
      "took 2.2939999103546143 seconds\n",
      "loss: 12.94\tbatch num: 11000/35074\n",
      "took 2.2140002250671387 seconds\n",
      "loss: 11.55\tbatch num: 12000/35074\n",
      "took 2.173999786376953 seconds\n",
      "loss: 11.82\tbatch num: 13000/35074\n",
      "took 2.181999444961548 seconds\n",
      "loss: 11.72\tbatch num: 14000/35074\n",
      "took 2.181000232696533 seconds\n",
      "loss: 11.59\tbatch num: 15000/35074\n",
      "took 2.172999858856201 seconds\n",
      "loss: 14.94\tbatch num: 16000/35074\n",
      "took 2.181999921798706 seconds\n",
      "loss: 10.07\tbatch num: 17000/35074\n",
      "took 2.1780006885528564 seconds\n",
      "loss: 11.89\tbatch num: 18000/35074\n",
      "took 2.1779990196228027 seconds\n",
      "loss: 10.86\tbatch num: 19000/35074\n",
      "took 2.1670005321502686 seconds\n",
      "loss: 11.50\tbatch num: 20000/35074\n",
      "took 2.1720004081726074 seconds\n",
      "loss: 13.30\tbatch num: 21000/35074\n",
      "took 2.190999984741211 seconds\n",
      "loss: 11.55\tbatch num: 22000/35074\n",
      "took 2.185999631881714 seconds\n",
      "loss: 11.14\tbatch num: 23000/35074\n",
      "took 2.161001682281494 seconds\n",
      "loss: 11.91\tbatch num: 24000/35074\n",
      "took 2.1659977436065674 seconds\n",
      "loss: 11.33\tbatch num: 25000/35074\n",
      "took 2.1649999618530273 seconds\n",
      "loss: 14.97\tbatch num: 26000/35074\n",
      "took 2.156000852584839 seconds\n",
      "loss: 13.44\tbatch num: 27000/35074\n",
      "took 2.158998489379883 seconds\n",
      "loss: 12.25\tbatch num: 28000/35074\n",
      "took 2.1960201263427734 seconds\n",
      "loss: 14.96\tbatch num: 29000/35074\n",
      "took 2.1999998092651367 seconds\n",
      "loss: 12.38\tbatch num: 30000/35074\n",
      "took 2.1879994869232178 seconds\n",
      "loss: 12.50\tbatch num: 31000/35074\n",
      "took 2.1810004711151123 seconds\n",
      "loss: 10.10\tbatch num: 32000/35074\n",
      "took 2.194000482559204 seconds\n",
      "loss: 15.76\tbatch num: 33000/35074\n",
      "took 2.1999993324279785 seconds\n",
      "loss: 13.72\tbatch num: 34000/35074\n",
      "took 2.2739996910095215 seconds\n",
      "loss: 13.43\tbatch num: 35000/35074\n",
      "took 2.263862371444702 seconds\n",
      "Epoch 1 took 77.92287969589233 seconds\n",
      "Average Loss: 31.290845036506653\n",
      "Starting epoch 2\n",
      "loss: 11.57\tbatch num: 0/35074\n",
      "took 0.06000113487243652 seconds\n",
      "loss: 11.13\tbatch num: 1000/35074\n",
      "took 2.242039680480957 seconds\n",
      "loss: 10.37\tbatch num: 2000/35074\n",
      "took 2.207000494003296 seconds\n",
      "loss: 12.83\tbatch num: 3000/35074\n",
      "took 2.1935505867004395 seconds\n",
      "loss: 13.74\tbatch num: 4000/35074\n",
      "took 2.271000385284424 seconds\n",
      "loss: 11.76\tbatch num: 5000/35074\n",
      "took 2.2729978561401367 seconds\n",
      "loss: 10.68\tbatch num: 6000/35074\n",
      "took 2.180999994277954 seconds\n",
      "loss: 13.55\tbatch num: 7000/35074\n",
      "took 2.193999767303467 seconds\n",
      "loss: 16.11\tbatch num: 8000/35074\n",
      "took 2.199000358581543 seconds\n",
      "loss: 11.99\tbatch num: 9000/35074\n",
      "took 2.1969997882843018 seconds\n",
      "loss: 15.12\tbatch num: 10000/35074\n",
      "took 2.1890008449554443 seconds\n",
      "loss: 11.16\tbatch num: 11000/35074\n",
      "took 2.2000019550323486 seconds\n",
      "loss: 13.19\tbatch num: 12000/35074\n",
      "took 2.1929972171783447 seconds\n",
      "loss: 11.67\tbatch num: 13000/35074\n",
      "took 2.186000347137451 seconds\n",
      "loss: 13.46\tbatch num: 14000/35074\n",
      "took 2.1889986991882324 seconds\n",
      "loss: 14.20\tbatch num: 15000/35074\n",
      "took 2.1760005950927734 seconds\n",
      "loss: 12.74\tbatch num: 16000/35074\n",
      "took 2.18999981880188 seconds\n",
      "loss: 13.72\tbatch num: 17000/35074\n",
      "took 2.159000873565674 seconds\n",
      "loss: 12.90\tbatch num: 18000/35074\n",
      "took 2.2069997787475586 seconds\n",
      "loss: 10.06\tbatch num: 19000/35074\n",
      "took 2.2210006713867188 seconds\n",
      "loss: 15.28\tbatch num: 20000/35074\n",
      "took 2.18800950050354 seconds\n",
      "loss: 13.18\tbatch num: 21000/35074\n",
      "took 2.2438666820526123 seconds\n",
      "loss: 9.50\tbatch num: 22000/35074\n",
      "took 2.190997838973999 seconds\n",
      "loss: 10.66\tbatch num: 23000/35074\n",
      "took 2.177000045776367 seconds\n",
      "loss: 10.85\tbatch num: 24000/35074\n",
      "took 2.1680002212524414 seconds\n",
      "loss: 12.69\tbatch num: 25000/35074\n",
      "took 2.2149999141693115 seconds\n",
      "loss: 14.51\tbatch num: 26000/35074\n",
      "took 2.1999993324279785 seconds\n",
      "loss: 14.41\tbatch num: 27000/35074\n",
      "took 2.294999361038208 seconds\n",
      "loss: 13.37\tbatch num: 28000/35074\n",
      "took 2.3020710945129395 seconds\n",
      "loss: 11.98\tbatch num: 29000/35074\n",
      "took 2.2790000438690186 seconds\n",
      "loss: 10.70\tbatch num: 30000/35074\n",
      "took 2.2930002212524414 seconds\n",
      "loss: 11.62\tbatch num: 31000/35074\n",
      "took 2.30299973487854 seconds\n",
      "loss: 12.39\tbatch num: 32000/35074\n",
      "took 2.2859995365142822 seconds\n",
      "loss: 12.81\tbatch num: 33000/35074\n",
      "took 2.2910006046295166 seconds\n",
      "loss: 8.95\tbatch num: 34000/35074\n",
      "took 2.279001474380493 seconds\n",
      "loss: 15.90\tbatch num: 35000/35074\n",
      "took 2.306138515472412 seconds\n",
      "Epoch 2 took 78.20067429542542 seconds\n",
      "Average Loss: 30.804879009723663\n",
      "Starting epoch 3\n",
      "loss: 11.09\tbatch num: 0/35074\n",
      "took 0.0650019645690918 seconds\n",
      "loss: 11.10\tbatch num: 1000/35074\n",
      "took 2.3059983253479004 seconds\n",
      "loss: 14.28\tbatch num: 2000/35074\n",
      "took 2.2740001678466797 seconds\n",
      "loss: 11.12\tbatch num: 3000/35074\n",
      "took 2.271000385284424 seconds\n",
      "loss: 13.08\tbatch num: 4000/35074\n",
      "took 2.279999017715454 seconds\n",
      "loss: 11.05\tbatch num: 5000/35074\n",
      "took 2.2830007076263428 seconds\n",
      "loss: 10.80\tbatch num: 6000/35074\n",
      "took 2.2760040760040283 seconds\n",
      "loss: 11.38\tbatch num: 7000/35074\n",
      "took 2.290438413619995 seconds\n",
      "loss: 12.99\tbatch num: 8000/35074\n",
      "took 2.2810003757476807 seconds\n",
      "loss: 13.30\tbatch num: 9000/35074\n",
      "took 2.268000841140747 seconds\n",
      "loss: 13.26\tbatch num: 10000/35074\n",
      "took 2.3012053966522217 seconds\n",
      "loss: 14.11\tbatch num: 11000/35074\n",
      "took 2.2840020656585693 seconds\n",
      "loss: 13.07\tbatch num: 12000/35074\n",
      "took 2.277960777282715 seconds\n",
      "loss: 12.37\tbatch num: 13000/35074\n",
      "took 2.2799999713897705 seconds\n",
      "loss: 14.22\tbatch num: 14000/35074\n",
      "took 2.300999641418457 seconds\n",
      "loss: 11.52\tbatch num: 15000/35074\n",
      "took 2.284998893737793 seconds\n",
      "loss: 10.68\tbatch num: 16000/35074\n",
      "took 2.414039134979248 seconds\n",
      "loss: 11.93\tbatch num: 17000/35074\n",
      "took 2.2589619159698486 seconds\n",
      "loss: 11.00\tbatch num: 18000/35074\n",
      "took 2.197000026702881 seconds\n",
      "loss: 13.07\tbatch num: 19000/35074\n",
      "took 2.308000087738037 seconds\n",
      "loss: 14.11\tbatch num: 20000/35074\n",
      "took 2.313999891281128 seconds\n",
      "loss: 12.65\tbatch num: 21000/35074\n",
      "took 2.317999839782715 seconds\n",
      "loss: 9.66\tbatch num: 22000/35074\n",
      "took 2.516000747680664 seconds\n",
      "loss: 14.73\tbatch num: 23000/35074\n",
      "took 2.399000406265259 seconds\n",
      "loss: 12.66\tbatch num: 24000/35074\n",
      "took 2.3889992237091064 seconds\n",
      "loss: 13.08\tbatch num: 25000/35074\n",
      "took 2.3889999389648438 seconds\n",
      "loss: 11.51\tbatch num: 26000/35074\n",
      "took 2.378999948501587 seconds\n",
      "loss: 11.94\tbatch num: 27000/35074\n",
      "took 2.4220030307769775 seconds\n",
      "loss: 12.62\tbatch num: 28000/35074\n",
      "took 2.5899975299835205 seconds\n",
      "loss: 15.91\tbatch num: 29000/35074\n",
      "took 2.3640010356903076 seconds\n",
      "loss: 14.40\tbatch num: 30000/35074\n",
      "took 2.5830299854278564 seconds\n",
      "loss: 12.33\tbatch num: 31000/35074\n",
      "took 2.421968698501587 seconds\n",
      "loss: 12.07\tbatch num: 32000/35074\n",
      "took 2.3889999389648438 seconds\n",
      "loss: 14.34\tbatch num: 33000/35074\n",
      "took 2.348999261856079 seconds\n",
      "loss: 13.50\tbatch num: 34000/35074\n",
      "took 2.9490034580230713 seconds\n",
      "loss: 11.19\tbatch num: 35000/35074\n",
      "took 3.9347519874572754 seconds\n",
      "Epoch 3 took 84.63136386871338 seconds\n",
      "Average Loss: 31.227736830711365\n",
      "Starting epoch 4\n",
      "loss: 13.89\tbatch num: 0/35074\n",
      "took 0.1159975528717041 seconds\n",
      "loss: 11.12\tbatch num: 1000/35074\n",
      "took 3.313000202178955 seconds\n",
      "loss: 9.28\tbatch num: 2000/35074\n",
      "took 3.3760011196136475 seconds\n",
      "loss: 12.19\tbatch num: 3000/35074\n",
      "took 3.4690001010894775 seconds\n",
      "loss: 12.14\tbatch num: 4000/35074\n",
      "took 3.474030017852783 seconds\n",
      "loss: 10.87\tbatch num: 5000/35074\n",
      "took 3.348971128463745 seconds\n",
      "loss: 15.07\tbatch num: 6000/35074\n",
      "took 3.643998384475708 seconds\n",
      "loss: 15.07\tbatch num: 7000/35074\n",
      "took 3.6309993267059326 seconds\n",
      "loss: 10.97\tbatch num: 8000/35074\n",
      "took 3.6110000610351562 seconds\n",
      "loss: 14.86\tbatch num: 9000/35074\n",
      "took 3.5860016345977783 seconds\n",
      "loss: 16.46\tbatch num: 10000/35074\n",
      "took 3.588999032974243 seconds\n",
      "loss: 12.10\tbatch num: 11000/35074\n",
      "took 3.537003993988037 seconds\n",
      "loss: 14.95\tbatch num: 12000/35074\n",
      "took 3.442996025085449 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.88\tbatch num: 13000/35074\n",
      "took 2.9670448303222656 seconds\n",
      "loss: 12.39\tbatch num: 14000/35074\n",
      "took 2.733955144882202 seconds\n",
      "loss: 9.54\tbatch num: 15000/35074\n",
      "took 3.294999122619629 seconds\n",
      "loss: 8.50\tbatch num: 16000/35074\n",
      "took 3.4190030097961426 seconds\n",
      "loss: 13.98\tbatch num: 17000/35074\n",
      "took 2.938997983932495 seconds\n",
      "loss: 11.74\tbatch num: 18000/35074\n",
      "took 3.1560044288635254 seconds\n",
      "loss: 10.42\tbatch num: 19000/35074\n",
      "took 3.211996078491211 seconds\n",
      "loss: 12.75\tbatch num: 20000/35074\n",
      "took 3.3030343055725098 seconds\n",
      "loss: 13.00\tbatch num: 21000/35074\n",
      "took 3.664668321609497 seconds\n",
      "loss: 13.04\tbatch num: 22000/35074\n",
      "took 3.493999719619751 seconds\n",
      "loss: 14.45\tbatch num: 23000/35074\n",
      "took 3.550999879837036 seconds\n",
      "loss: 13.85\tbatch num: 24000/35074\n",
      "took 3.585998773574829 seconds\n",
      "loss: 11.69\tbatch num: 25000/35074\n",
      "took 3.5340027809143066 seconds\n",
      "loss: 11.82\tbatch num: 26000/35074\n",
      "took 3.4759981632232666 seconds\n",
      "loss: 12.52\tbatch num: 27000/35074\n",
      "took 4.3420023918151855 seconds\n",
      "loss: 13.46\tbatch num: 28000/35074\n",
      "took 3.5249974727630615 seconds\n",
      "loss: 14.19\tbatch num: 29000/35074\n",
      "took 3.3540029525756836 seconds\n",
      "loss: 7.28\tbatch num: 30000/35074\n",
      "took 3.501000165939331 seconds\n",
      "loss: 11.22\tbatch num: 31000/35074\n",
      "took 3.5519983768463135 seconds\n",
      "loss: 10.98\tbatch num: 32000/35074\n",
      "took 3.8129987716674805 seconds\n",
      "loss: 14.41\tbatch num: 33000/35074\n",
      "took 3.5750045776367188 seconds\n",
      "loss: 15.61\tbatch num: 34000/35074\n",
      "took 3.294994831085205 seconds\n",
      "loss: 9.55\tbatch num: 35000/35074\n",
      "took 3.6290016174316406 seconds\n",
      "Epoch 4 took 121.42570042610168 seconds\n",
      "Average Loss: 30.88699561357498\n",
      "Starting epoch 5\n",
      "loss: 14.50\tbatch num: 0/35074\n",
      "took 0.09699845314025879 seconds\n",
      "loss: 14.27\tbatch num: 1000/35074\n",
      "took 3.3750033378601074 seconds\n",
      "loss: 11.45\tbatch num: 2000/35074\n",
      "took 3.2460014820098877 seconds\n",
      "loss: 11.91\tbatch num: 3000/35074\n",
      "took 2.986995220184326 seconds\n",
      "loss: 12.15\tbatch num: 4000/35074\n",
      "took 3.451002597808838 seconds\n",
      "loss: 10.68\tbatch num: 5000/35074\n",
      "took 3.018001079559326 seconds\n",
      "loss: 10.89\tbatch num: 6000/35074\n",
      "took 3.1019997596740723 seconds\n",
      "loss: 11.21\tbatch num: 7000/35074\n",
      "took 3.0369977951049805 seconds\n",
      "loss: 9.53\tbatch num: 8000/35074\n",
      "took 3.0980005264282227 seconds\n",
      "loss: 12.26\tbatch num: 9000/35074\n",
      "took 2.9349992275238037 seconds\n",
      "loss: 10.59\tbatch num: 10000/35074\n",
      "took 3.160001277923584 seconds\n",
      "loss: 13.32\tbatch num: 11000/35074\n",
      "took 3.1199989318847656 seconds\n",
      "loss: 13.93\tbatch num: 12000/35074\n",
      "took 3.2290008068084717 seconds\n",
      "loss: 15.94\tbatch num: 13000/35074\n",
      "took 3.115000009536743 seconds\n",
      "loss: 15.08\tbatch num: 14000/35074\n",
      "took 2.9610023498535156 seconds\n",
      "loss: 12.42\tbatch num: 15000/35074\n",
      "took 2.891000747680664 seconds\n",
      "loss: 13.45\tbatch num: 16000/35074\n",
      "took 2.8549983501434326 seconds\n",
      "loss: 9.57\tbatch num: 17000/35074\n",
      "took 2.9099998474121094 seconds\n",
      "loss: 10.78\tbatch num: 18000/35074\n",
      "took 2.885997772216797 seconds\n",
      "loss: 11.54\tbatch num: 19000/35074\n",
      "took 2.9090042114257812 seconds\n",
      "loss: 10.47\tbatch num: 20000/35074\n",
      "took 2.8470261096954346 seconds\n",
      "loss: 12.01\tbatch num: 21000/35074\n",
      "took 2.8239734172821045 seconds\n",
      "loss: 16.52\tbatch num: 22000/35074\n",
      "took 2.86199688911438 seconds\n",
      "loss: 12.91\tbatch num: 23000/35074\n",
      "took 2.7659995555877686 seconds\n",
      "loss: 13.35\tbatch num: 24000/35074\n",
      "took 2.779001235961914 seconds\n",
      "loss: 12.15\tbatch num: 25000/35074\n",
      "took 2.7830018997192383 seconds\n",
      "loss: 12.25\tbatch num: 26000/35074\n",
      "took 2.7939963340759277 seconds\n",
      "loss: 11.12\tbatch num: 27000/35074\n",
      "took 2.777001142501831 seconds\n",
      "loss: 12.50\tbatch num: 28000/35074\n",
      "took 2.8720028400421143 seconds\n",
      "loss: 13.16\tbatch num: 29000/35074\n",
      "took 2.863999128341675 seconds\n",
      "loss: 11.00\tbatch num: 30000/35074\n",
      "took 2.7329978942871094 seconds\n",
      "loss: 12.55\tbatch num: 31000/35074\n",
      "took 2.7000014781951904 seconds\n",
      "loss: 9.45\tbatch num: 32000/35074\n",
      "took 2.714998722076416 seconds\n",
      "loss: 10.99\tbatch num: 33000/35074\n",
      "took 2.7759993076324463 seconds\n",
      "loss: 13.90\tbatch num: 34000/35074\n",
      "took 2.6890017986297607 seconds\n",
      "loss: 14.15\tbatch num: 35000/35074\n",
      "took 2.7460010051727295 seconds\n",
      "Epoch 5 took 103.21100068092346 seconds\n",
      "Average Loss: 30.671087622642517\n",
      "Starting epoch 6\n",
      "loss: 15.54\tbatch num: 0/35074\n",
      "took 0.0800008773803711 seconds\n",
      "loss: 12.63\tbatch num: 1000/35074\n",
      "took 2.875000238418579 seconds\n",
      "loss: 10.13\tbatch num: 2000/35074\n",
      "took 2.715998888015747 seconds\n",
      "loss: 14.09\tbatch num: 3000/35074\n",
      "took 2.7330007553100586 seconds\n",
      "loss: 11.41\tbatch num: 4000/35074\n",
      "took 2.772000789642334 seconds\n",
      "loss: 11.46\tbatch num: 5000/35074\n",
      "took 2.7489984035491943 seconds\n",
      "loss: 11.44\tbatch num: 6000/35074\n",
      "took 2.774000883102417 seconds\n",
      "loss: 11.30\tbatch num: 7000/35074\n",
      "took 2.790001630783081 seconds\n",
      "loss: 11.47\tbatch num: 8000/35074\n",
      "took 2.76299786567688 seconds\n",
      "loss: 13.00\tbatch num: 9000/35074\n",
      "took 2.703002452850342 seconds\n",
      "loss: 14.13\tbatch num: 10000/35074\n",
      "took 2.6767656803131104 seconds\n",
      "loss: 12.82\tbatch num: 11000/35074\n",
      "took 2.767998456954956 seconds\n",
      "loss: 14.03\tbatch num: 12000/35074\n",
      "took 2.817002058029175 seconds\n",
      "loss: 12.36\tbatch num: 13000/35074\n",
      "took 2.750279664993286 seconds\n",
      "loss: 11.68\tbatch num: 14000/35074\n",
      "took 2.688995838165283 seconds\n",
      "loss: 15.69\tbatch num: 15000/35074\n",
      "took 2.734003782272339 seconds\n",
      "loss: 14.75\tbatch num: 16000/35074\n",
      "took 2.7297310829162598 seconds\n",
      "loss: 13.98\tbatch num: 17000/35074\n",
      "took 2.785000801086426 seconds\n",
      "loss: 10.88\tbatch num: 18000/35074\n",
      "took 2.7320010662078857 seconds\n",
      "loss: 10.94\tbatch num: 19000/35074\n",
      "took 2.767998456954956 seconds\n",
      "loss: 13.24\tbatch num: 20000/35074\n",
      "took 2.7350003719329834 seconds\n",
      "loss: 10.96\tbatch num: 21000/35074\n",
      "took 2.7609996795654297 seconds\n",
      "loss: 11.81\tbatch num: 22000/35074\n",
      "took 2.7680001258850098 seconds\n",
      "loss: 15.78\tbatch num: 23000/35074\n",
      "took 2.7410004138946533 seconds\n",
      "loss: 10.64\tbatch num: 24000/35074\n",
      "took 2.7790002822875977 seconds\n",
      "loss: 12.52\tbatch num: 25000/35074\n",
      "took 2.7619998455047607 seconds\n",
      "loss: 14.30\tbatch num: 26000/35074\n",
      "took 2.7270002365112305 seconds\n",
      "loss: 11.91\tbatch num: 27000/35074\n",
      "took 2.654999017715454 seconds\n",
      "loss: 10.40\tbatch num: 28000/35074\n",
      "took 2.612008571624756 seconds\n",
      "loss: 13.52\tbatch num: 29000/35074\n",
      "took 2.603989839553833 seconds\n",
      "loss: 10.15\tbatch num: 30000/35074\n",
      "took 2.7480008602142334 seconds\n",
      "loss: 15.09\tbatch num: 31000/35074\n",
      "took 2.792999505996704 seconds\n",
      "loss: 11.10\tbatch num: 32000/35074\n",
      "took 2.72300124168396 seconds\n",
      "loss: 14.09\tbatch num: 33000/35074\n",
      "took 2.8050012588500977 seconds\n",
      "loss: 12.06\tbatch num: 34000/35074\n",
      "took 3.0400004386901855 seconds\n",
      "loss: 12.53\tbatch num: 35000/35074\n",
      "took 3.009000539779663 seconds\n",
      "Epoch 6 took 97.00977849960327 seconds\n",
      "Average Loss: 30.935466051101685\n",
      "Starting epoch 7\n",
      "loss: 10.34\tbatch num: 0/35074\n",
      "took 0.09600019454956055 seconds\n",
      "loss: 11.19\tbatch num: 1000/35074\n",
      "took 3.632002592086792 seconds\n",
      "loss: 11.10\tbatch num: 2000/35074\n",
      "took 3.2769973278045654 seconds\n",
      "loss: 14.06\tbatch num: 3000/35074\n",
      "took 3.4459996223449707 seconds\n",
      "loss: 12.28\tbatch num: 4000/35074\n",
      "took 3.19100022315979 seconds\n",
      "loss: 12.77\tbatch num: 5000/35074\n",
      "took 3.2240025997161865 seconds\n",
      "loss: 9.46\tbatch num: 6000/35074\n",
      "took 3.302997350692749 seconds\n",
      "loss: 11.90\tbatch num: 7000/35074\n",
      "took 3.417001485824585 seconds\n",
      "loss: 8.59\tbatch num: 8000/35074\n",
      "took 3.3630027770996094 seconds\n",
      "loss: 12.79\tbatch num: 9000/35074\n",
      "took 3.2889981269836426 seconds\n",
      "loss: 15.67\tbatch num: 10000/35074\n",
      "took 3.3740017414093018 seconds\n",
      "loss: 12.78\tbatch num: 11000/35074\n",
      "took 3.097996711730957 seconds\n",
      "loss: 11.17\tbatch num: 12000/35074\n",
      "took 2.981001853942871 seconds\n",
      "loss: 15.93\tbatch num: 13000/35074\n",
      "took 2.9509973526000977 seconds\n",
      "loss: 10.90\tbatch num: 14000/35074\n",
      "took 2.880999803543091 seconds\n",
      "loss: 14.64\tbatch num: 15000/35074\n",
      "took 2.818000316619873 seconds\n",
      "loss: 11.20\tbatch num: 16000/35074\n",
      "took 2.667999744415283 seconds\n",
      "loss: 14.48\tbatch num: 17000/35074\n",
      "took 2.3100008964538574 seconds\n",
      "loss: 10.05\tbatch num: 18000/35074\n",
      "took 2.307999610900879 seconds\n",
      "loss: 12.05\tbatch num: 19000/35074\n",
      "took 2.3160014152526855 seconds\n",
      "loss: 12.34\tbatch num: 20000/35074\n",
      "took 2.283996820449829 seconds\n",
      "loss: 14.05\tbatch num: 21000/35074\n",
      "took 2.309001922607422 seconds\n",
      "loss: 12.53\tbatch num: 22000/35074\n",
      "took 2.311001777648926 seconds\n",
      "loss: 13.34\tbatch num: 23000/35074\n",
      "took 2.3079993724823 seconds\n",
      "loss: 11.76\tbatch num: 24000/35074\n",
      "took 2.317000150680542 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.71\tbatch num: 25000/35074\n",
      "took 2.2959988117218018 seconds\n",
      "loss: 11.25\tbatch num: 26000/35074\n",
      "took 2.296001434326172 seconds\n",
      "loss: 13.62\tbatch num: 27000/35074\n",
      "took 2.3170340061187744 seconds\n",
      "loss: 10.77\tbatch num: 28000/35074\n",
      "took 2.330963611602783 seconds\n",
      "loss: 12.03\tbatch num: 29000/35074\n",
      "took 2.3030362129211426 seconds\n",
      "loss: 11.12\tbatch num: 30000/35074\n",
      "took 2.3109664916992188 seconds\n",
      "loss: 9.31\tbatch num: 31000/35074\n",
      "took 2.294999837875366 seconds\n",
      "loss: 11.46\tbatch num: 32000/35074\n",
      "took 2.3109993934631348 seconds\n",
      "loss: 11.69\tbatch num: 33000/35074\n",
      "took 2.2937746047973633 seconds\n",
      "loss: 9.53\tbatch num: 34000/35074\n",
      "took 2.270167112350464 seconds\n",
      "loss: 11.37\tbatch num: 35000/35074\n",
      "took 2.2850401401519775 seconds\n",
      "Epoch 7 took 95.03298473358154 seconds\n",
      "Average Loss: 31.27614003419876\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "model = TestModel().cuda()\n",
    "loss_fn = torch.nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, model, loss_fn)\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"testmodel.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d8ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is within five pins 30.7% of the time\n",
      "Model is within ten pins 50.8% of the time\n",
      "Model is within fifteen pins 67.5% of the time\n"
     ]
    }
   ],
   "source": [
    "fivePins = 0\n",
    "tenPins = 0\n",
    "fifteenPins = 0\n",
    "total = 0\n",
    "for x in range(len(testData)):\n",
    "    total += 1\n",
    "    pred = model(testData[x][0])\n",
    "    diff = abs(pred - testData[x][1])\n",
    "    if diff <= 5:\n",
    "        fivePins += 1\n",
    "    if diff <= 10:\n",
    "        tenPins += 1\n",
    "    if diff <= 15:\n",
    "        fifteenPins += 1\n",
    "print(f\"Model is within five pins {fivePins * 100 / total}% of the time\")\n",
    "print(f\"Model is within ten pins {tenPins * 100 / total}% of the time\")\n",
    "print(f\"Model is within fifteen pins {fifteenPins * 100 / total}% of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27eecb-7708-47d7-b8c1-fe6f22dc543e",
   "metadata": {},
   "source": [
    "The R-squared for most of these models hovers around 0.73, which is pretty good :)\n",
    "\n",
    "\n",
    "Model with L1 Loss, adam at lr=0.0005, batchsize = 64, and 8 epochs:\n",
    "    Model is within five pins 30.5% of the time\n",
    "    Model is within ten pins 50.4% of the time\n",
    "    Model is within fifteen pins 65.8% of the time\n",
    "\n",
    "Model with L1 Loss, adam at lr=0.001, batchsize = 64, and 8 epochs:\n",
    "    Model is within five pins 30.2% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 66.7% of the time\n",
    "\n",
    "More neurons Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 29.5% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.9% of the time\n",
    "\n",
    "less neurons Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 30.6% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 67.5% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 31.8% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with MSE Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 29.1% of the time\n",
    "    Model is within ten pins 49.8% of the time\n",
    "    Model is within fifteen pins 66.5% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, adam at lr=0.00005, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 31.8% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.0001, batchsize = 32, and 8 epochs:\\\n",
    "    Model is within five pins 30.0% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 16, and 8 epochs:\\\n",
    "    Model is within five pins 29.5% of the tim e\n",
    "    Model is within ten pins 50.6% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 4, and 8 epochs:\n",
    "    Model is within five pins 31.3% of the time\n",
    "    Model is within ten pins 52.2% of the time\n",
    "    Model is within fifteen pins 68.2% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 1, and 8 epochs:\n",
    "    Model is within five pins 31.0% of the time\n",
    "    Model is within ten pins 52.9% of the time\n",
    "    Model is within fifteen pins 67.7% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cfb320c-bab8-4409-b4b2-4e5014ef2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190.797\n"
     ]
    }
   ],
   "source": [
    "#first, calculate mean score. just gonna manually do this by iterating over dataset\n",
    "\"\"\"\n",
    "tSum = 0\n",
    "tCount = 0\n",
    "for row in testData:\n",
    "    tCount += 1\n",
    "    tSum += int(row[1])\n",
    "    if not tCount % 100000:\n",
    "        print(f\"finished with {tCount}\")\n",
    "mean = tSum/tCount\n",
    "print(mean)\n",
    "\"\"\"\n",
    "testMean=190.797\n",
    "trainMean=191.9994600624403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914576fc-83a6-41ce-80e7-964c6b812a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SST = 0\n",
    "tCount = 0\n",
    "for row in testData:\n",
    "    tCount += 1\n",
    "    SST += (int(row[1]) - mean) ** 2\n",
    "    if not tCount % 50000:\n",
    "        print(tCount)\n",
    "print(SST)\n",
    "\"\"\"\n",
    "testSST = 949951.791\n",
    "trainSST = 1108731521.6719563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "824181c4-cc27-4fdc-888c-3a8fc4a2e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([273146.7188], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#calculate SSE, which sums of squared residuals (errors)\n",
    "SSE = 0\n",
    "tCount = 0\n",
    "for row in testData:\n",
    "    tCount += 1\n",
    "    pred = model(row[0])\n",
    "    resid = pred - row[1]\n",
    "    SSE += resid ** 2\n",
    "    if not tCount % 50000:\n",
    "        print(tCount)\n",
    "print(SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8289a485-2f35-4782-9012-d3f61b5a1580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311], device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#Model R2 on model data is about 0.7311\n",
    "#Model R2 on test data is about 0.7125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021e57e7-035f-4487-91c9-454679154d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7125], device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(1 - SSE/testSST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
