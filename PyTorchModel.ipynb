{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83abf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4533b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49651224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that pulls data from file as requested\n",
    "\"\"\" This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.f.seek(idx * 27)\n",
    "        gameData = self.f.read(27)\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446fe4d5-830a-4a7e-872e-5bcd450b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that loads all data into GPU memory\n",
    "#this works significantly better but we are hard limited by vram. maybe try to look into memory pinning more\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        self.f.seek(0)\n",
    "        # build entire array into memory\n",
    "        self.inputArray=[]\n",
    "        self.finalScoreArray=[]\n",
    "        for v in range(self.length):\n",
    "            gameData = self.f.read(27)\n",
    "            tempArray = []\n",
    "            for x in range(25):\n",
    "                tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "            self.finalScoreArray.append(gameData[-1] * 256 + gameData[-2])\n",
    "            self.inputArray.append([float(v) for v in tempArray][:120])\n",
    "            if not v % 10000:\n",
    "                print(f\"loaded {v} our of {self.length}\")\n",
    "        self.inputTensor = torch.tensor(self.inputArray).to(\"cuda\")\n",
    "        self.outputTensor = torch.tensor(self.finalScoreArray).to(\"cuda\")\n",
    "        self.inputArray = []\n",
    "        self.finalScoreArray = []\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputTensor[idx], self.outputTensor[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16999c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.relu_stack(x)\n",
    "        output = torch.nn.Sigmoid()(logits)\n",
    "        return output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    totalBatches = len(dataloader)\n",
    "    model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction + loss\n",
    "        prediction = model(x).squeeze(1)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            print(f\"loss: {loss.item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "            print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval() # need to look into what this does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    print(f\"Average Loss: {test_loss/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0211bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed9adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 0 our of 1122352\n",
      "loaded 10000 our of 1122352\n",
      "loaded 20000 our of 1122352\n",
      "loaded 30000 our of 1122352\n",
      "loaded 40000 our of 1122352\n",
      "loaded 50000 our of 1122352\n",
      "loaded 60000 our of 1122352\n",
      "loaded 70000 our of 1122352\n",
      "loaded 80000 our of 1122352\n",
      "loaded 90000 our of 1122352\n",
      "loaded 100000 our of 1122352\n",
      "loaded 110000 our of 1122352\n",
      "loaded 120000 our of 1122352\n",
      "loaded 130000 our of 1122352\n",
      "loaded 140000 our of 1122352\n",
      "loaded 150000 our of 1122352\n",
      "loaded 160000 our of 1122352\n",
      "loaded 170000 our of 1122352\n",
      "loaded 180000 our of 1122352\n",
      "loaded 190000 our of 1122352\n",
      "loaded 200000 our of 1122352\n",
      "loaded 210000 our of 1122352\n",
      "loaded 220000 our of 1122352\n",
      "loaded 230000 our of 1122352\n",
      "loaded 240000 our of 1122352\n",
      "loaded 250000 our of 1122352\n",
      "loaded 260000 our of 1122352\n",
      "loaded 270000 our of 1122352\n",
      "loaded 280000 our of 1122352\n",
      "loaded 290000 our of 1122352\n",
      "loaded 300000 our of 1122352\n",
      "loaded 310000 our of 1122352\n",
      "loaded 320000 our of 1122352\n",
      "loaded 330000 our of 1122352\n",
      "loaded 340000 our of 1122352\n",
      "loaded 350000 our of 1122352\n",
      "loaded 360000 our of 1122352\n",
      "loaded 370000 our of 1122352\n",
      "loaded 380000 our of 1122352\n",
      "loaded 390000 our of 1122352\n",
      "loaded 400000 our of 1122352\n",
      "loaded 410000 our of 1122352\n",
      "loaded 420000 our of 1122352\n",
      "loaded 430000 our of 1122352\n",
      "loaded 440000 our of 1122352\n",
      "loaded 450000 our of 1122352\n",
      "loaded 460000 our of 1122352\n",
      "loaded 470000 our of 1122352\n",
      "loaded 480000 our of 1122352\n",
      "loaded 490000 our of 1122352\n",
      "loaded 500000 our of 1122352\n",
      "loaded 510000 our of 1122352\n",
      "loaded 520000 our of 1122352\n",
      "loaded 530000 our of 1122352\n",
      "loaded 540000 our of 1122352\n",
      "loaded 550000 our of 1122352\n",
      "loaded 560000 our of 1122352\n",
      "loaded 570000 our of 1122352\n",
      "loaded 580000 our of 1122352\n",
      "loaded 590000 our of 1122352\n",
      "loaded 600000 our of 1122352\n",
      "loaded 610000 our of 1122352\n",
      "loaded 620000 our of 1122352\n",
      "loaded 630000 our of 1122352\n",
      "loaded 640000 our of 1122352\n",
      "loaded 650000 our of 1122352\n",
      "loaded 660000 our of 1122352\n",
      "loaded 670000 our of 1122352\n",
      "loaded 680000 our of 1122352\n",
      "loaded 690000 our of 1122352\n",
      "loaded 700000 our of 1122352\n",
      "loaded 710000 our of 1122352\n",
      "loaded 720000 our of 1122352\n",
      "loaded 730000 our of 1122352\n",
      "loaded 740000 our of 1122352\n",
      "loaded 750000 our of 1122352\n",
      "loaded 760000 our of 1122352\n",
      "loaded 770000 our of 1122352\n",
      "loaded 780000 our of 1122352\n",
      "loaded 790000 our of 1122352\n",
      "loaded 800000 our of 1122352\n",
      "loaded 810000 our of 1122352\n",
      "loaded 820000 our of 1122352\n",
      "loaded 830000 our of 1122352\n",
      "loaded 840000 our of 1122352\n",
      "loaded 850000 our of 1122352\n",
      "loaded 860000 our of 1122352\n",
      "loaded 870000 our of 1122352\n",
      "loaded 880000 our of 1122352\n",
      "loaded 890000 our of 1122352\n",
      "loaded 900000 our of 1122352\n",
      "loaded 910000 our of 1122352\n",
      "loaded 920000 our of 1122352\n",
      "loaded 930000 our of 1122352\n",
      "loaded 940000 our of 1122352\n",
      "loaded 950000 our of 1122352\n",
      "loaded 960000 our of 1122352\n",
      "loaded 970000 our of 1122352\n",
      "loaded 980000 our of 1122352\n",
      "loaded 990000 our of 1122352\n",
      "loaded 1000000 our of 1122352\n",
      "loaded 1010000 our of 1122352\n",
      "loaded 1020000 our of 1122352\n",
      "loaded 1030000 our of 1122352\n",
      "loaded 1040000 our of 1122352\n",
      "loaded 1050000 our of 1122352\n",
      "loaded 1060000 our of 1122352\n",
      "loaded 1070000 our of 1122352\n",
      "loaded 1080000 our of 1122352\n",
      "loaded 1090000 our of 1122352\n",
      "loaded 1100000 our of 1122352\n",
      "loaded 1110000 our of 1122352\n",
      "loaded 1120000 our of 1122352\n",
      "loaded 0 our of 1000\n"
     ]
    }
   ],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74655ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test to see if pinning is working\n",
    "for batch_ndx, sample in enumerate(trainDataLoader):\n",
    "    if batch_ndx > 5:\n",
    "        break\n",
    "    print(sample[1].is_pinned())\n",
    "    print(sample[1].is_cuda)\n",
    "print(len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8191c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "loss: 34.86\tbatch num: 0/35074\n",
      "took 17.76923894882202 seconds\n",
      "loss: 11.46\tbatch num: 1000/35074\n",
      "took 1.9395813941955566 seconds\n",
      "loss: 15.26\tbatch num: 2000/35074\n",
      "took 1.9139950275421143 seconds\n",
      "loss: 14.10\tbatch num: 3000/35074\n",
      "took 1.917661428451538 seconds\n",
      "loss: 12.36\tbatch num: 4000/35074\n",
      "took 1.9121284484863281 seconds\n",
      "loss: 10.97\tbatch num: 5000/35074\n",
      "took 1.9018332958221436 seconds\n",
      "loss: 13.84\tbatch num: 6000/35074\n",
      "took 1.908200740814209 seconds\n",
      "loss: 15.09\tbatch num: 7000/35074\n",
      "took 1.9136970043182373 seconds\n",
      "loss: 11.22\tbatch num: 8000/35074\n",
      "took 1.9021010398864746 seconds\n",
      "loss: 12.83\tbatch num: 9000/35074\n",
      "took 1.9036426544189453 seconds\n",
      "loss: 10.95\tbatch num: 10000/35074\n",
      "took 1.8979237079620361 seconds\n",
      "loss: 11.05\tbatch num: 11000/35074\n",
      "took 1.9032649993896484 seconds\n",
      "loss: 10.96\tbatch num: 12000/35074\n",
      "took 1.9398694038391113 seconds\n",
      "loss: 12.76\tbatch num: 13000/35074\n",
      "took 1.9101498126983643 seconds\n",
      "loss: 13.79\tbatch num: 14000/35074\n",
      "took 1.9042103290557861 seconds\n",
      "loss: 15.41\tbatch num: 15000/35074\n",
      "took 1.899294376373291 seconds\n",
      "loss: 13.50\tbatch num: 16000/35074\n",
      "took 1.917294979095459 seconds\n",
      "loss: 12.51\tbatch num: 17000/35074\n",
      "took 1.9257032871246338 seconds\n",
      "loss: 12.11\tbatch num: 18000/35074\n",
      "took 1.9241225719451904 seconds\n",
      "loss: 11.07\tbatch num: 19000/35074\n",
      "took 1.9110000133514404 seconds\n",
      "loss: 14.78\tbatch num: 20000/35074\n",
      "took 1.9014651775360107 seconds\n",
      "loss: 11.35\tbatch num: 21000/35074\n",
      "took 1.8907532691955566 seconds\n",
      "loss: 16.40\tbatch num: 22000/35074\n",
      "took 1.9293594360351562 seconds\n",
      "loss: 12.37\tbatch num: 23000/35074\n",
      "took 1.9234819412231445 seconds\n",
      "loss: 12.66\tbatch num: 24000/35074\n",
      "took 1.8908932209014893 seconds\n",
      "loss: 10.83\tbatch num: 25000/35074\n",
      "took 1.9193167686462402 seconds\n",
      "loss: 13.75\tbatch num: 26000/35074\n",
      "took 1.8966116905212402 seconds\n",
      "loss: 11.52\tbatch num: 27000/35074\n",
      "took 1.912060022354126 seconds\n",
      "loss: 8.57\tbatch num: 28000/35074\n",
      "took 2.030634880065918 seconds\n",
      "loss: 13.67\tbatch num: 29000/35074\n",
      "took 1.9651010036468506 seconds\n",
      "loss: 13.76\tbatch num: 30000/35074\n",
      "took 1.9148919582366943 seconds\n",
      "loss: 17.51\tbatch num: 31000/35074\n",
      "took 1.933485507965088 seconds\n",
      "loss: 11.26\tbatch num: 32000/35074\n",
      "took 1.9299921989440918 seconds\n",
      "loss: 8.82\tbatch num: 33000/35074\n",
      "took 1.9327311515808105 seconds\n",
      "loss: 14.61\tbatch num: 34000/35074\n",
      "took 1.987999439239502 seconds\n",
      "loss: 14.00\tbatch num: 35000/35074\n",
      "took 1.9729619026184082 seconds\n",
      "Epoch 0 took 85.25365281105042 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 30.749379575252533\n",
      "Starting epoch 1\n",
      "loss: 12.77\tbatch num: 0/35074\n",
      "took 0.03699803352355957 seconds\n",
      "loss: 8.78\tbatch num: 1000/35074\n",
      "took 1.8810009956359863 seconds\n",
      "loss: 11.14\tbatch num: 2000/35074\n",
      "took 1.8700006008148193 seconds\n",
      "loss: 13.06\tbatch num: 3000/35074\n",
      "took 1.878000259399414 seconds\n",
      "loss: 15.17\tbatch num: 4000/35074\n",
      "took 1.858999252319336 seconds\n",
      "loss: 12.75\tbatch num: 5000/35074\n",
      "took 1.8670001029968262 seconds\n",
      "loss: 11.53\tbatch num: 6000/35074\n",
      "took 1.876999855041504 seconds\n",
      "loss: 14.17\tbatch num: 7000/35074\n",
      "took 1.8609998226165771 seconds\n",
      "loss: 10.93\tbatch num: 8000/35074\n",
      "took 1.8810005187988281 seconds\n",
      "loss: 11.58\tbatch num: 9000/35074\n",
      "took 1.86799955368042 seconds\n",
      "loss: 13.65\tbatch num: 10000/35074\n",
      "took 1.8489999771118164 seconds\n",
      "loss: 15.98\tbatch num: 11000/35074\n",
      "took 1.8620004653930664 seconds\n",
      "loss: 13.93\tbatch num: 12000/35074\n",
      "took 1.9570000171661377 seconds\n",
      "loss: 16.58\tbatch num: 13000/35074\n",
      "took 1.875 seconds\n",
      "loss: 9.78\tbatch num: 14000/35074\n",
      "took 1.9019999504089355 seconds\n",
      "loss: 9.15\tbatch num: 15000/35074\n",
      "took 1.8510005474090576 seconds\n",
      "loss: 13.19\tbatch num: 16000/35074\n",
      "took 1.8699994087219238 seconds\n",
      "loss: 11.01\tbatch num: 17000/35074\n",
      "took 1.8570001125335693 seconds\n",
      "loss: 11.91\tbatch num: 18000/35074\n",
      "took 1.8559997081756592 seconds\n",
      "loss: 9.86\tbatch num: 19000/35074\n",
      "took 1.8659999370574951 seconds\n",
      "loss: 18.16\tbatch num: 20000/35074\n",
      "took 1.8529996871948242 seconds\n",
      "loss: 10.90\tbatch num: 21000/35074\n",
      "took 1.8550002574920654 seconds\n",
      "loss: 14.64\tbatch num: 22000/35074\n",
      "took 1.8680002689361572 seconds\n",
      "loss: 10.23\tbatch num: 23000/35074\n",
      "took 1.8580005168914795 seconds\n",
      "loss: 9.38\tbatch num: 24000/35074\n",
      "took 1.865999460220337 seconds\n",
      "loss: 11.31\tbatch num: 25000/35074\n",
      "took 1.873000144958496 seconds\n",
      "loss: 12.69\tbatch num: 26000/35074\n",
      "took 1.8550000190734863 seconds\n",
      "loss: 10.05\tbatch num: 27000/35074\n",
      "took 1.863999605178833 seconds\n",
      "loss: 9.82\tbatch num: 28000/35074\n",
      "took 1.8580000400543213 seconds\n",
      "loss: 14.46\tbatch num: 29000/35074\n",
      "took 1.8649990558624268 seconds\n",
      "loss: 12.53\tbatch num: 30000/35074\n",
      "took 1.8620007038116455 seconds\n",
      "loss: 12.26\tbatch num: 31000/35074\n",
      "took 1.8450002670288086 seconds\n",
      "loss: 14.89\tbatch num: 32000/35074\n",
      "took 1.8489999771118164 seconds\n",
      "loss: 11.94\tbatch num: 33000/35074\n",
      "took 1.8370001316070557 seconds\n",
      "loss: 14.39\tbatch num: 34000/35074\n",
      "took 1.8420000076293945 seconds\n",
      "loss: 13.14\tbatch num: 35000/35074\n",
      "took 1.860999345779419 seconds\n",
      "Epoch 1 took 65.51799845695496 seconds\n",
      "Average Loss: 30.80962598323822\n",
      "Starting epoch 2\n",
      "loss: 10.30\tbatch num: 0/35074\n",
      "took 0.037998199462890625 seconds\n",
      "loss: 18.01\tbatch num: 1000/35074\n",
      "took 1.886000633239746 seconds\n",
      "loss: 13.56\tbatch num: 2000/35074\n",
      "took 1.8420002460479736 seconds\n",
      "loss: 8.88\tbatch num: 3000/35074\n",
      "took 1.8680000305175781 seconds\n",
      "loss: 14.69\tbatch num: 4000/35074\n",
      "took 1.8710002899169922 seconds\n",
      "loss: 13.75\tbatch num: 5000/35074\n",
      "took 1.857999324798584 seconds\n",
      "loss: 10.41\tbatch num: 6000/35074\n",
      "took 1.8660001754760742 seconds\n",
      "loss: 13.02\tbatch num: 7000/35074\n",
      "took 1.8429994583129883 seconds\n",
      "loss: 9.76\tbatch num: 8000/35074\n",
      "took 1.8530001640319824 seconds\n",
      "loss: 9.61\tbatch num: 9000/35074\n",
      "took 1.8479998111724854 seconds\n",
      "loss: 9.70\tbatch num: 10000/35074\n",
      "took 1.856001377105713 seconds\n",
      "loss: 12.51\tbatch num: 11000/35074\n",
      "took 1.8699994087219238 seconds\n",
      "loss: 12.69\tbatch num: 12000/35074\n",
      "took 1.841998815536499 seconds\n",
      "loss: 11.65\tbatch num: 13000/35074\n",
      "took 1.8490016460418701 seconds\n",
      "loss: 13.66\tbatch num: 14000/35074\n",
      "took 1.8499999046325684 seconds\n",
      "loss: 11.62\tbatch num: 15000/35074\n",
      "took 1.8469996452331543 seconds\n",
      "loss: 14.50\tbatch num: 16000/35074\n",
      "took 1.865999698638916 seconds\n",
      "loss: 11.50\tbatch num: 17000/35074\n",
      "took 1.8499999046325684 seconds\n",
      "loss: 14.54\tbatch num: 18000/35074\n",
      "took 1.8339991569519043 seconds\n",
      "loss: 11.66\tbatch num: 19000/35074\n",
      "took 1.8550009727478027 seconds\n",
      "loss: 12.44\tbatch num: 20000/35074\n",
      "took 1.8500001430511475 seconds\n",
      "loss: 6.40\tbatch num: 21000/35074\n",
      "took 1.8359997272491455 seconds\n",
      "loss: 9.20\tbatch num: 22000/35074\n",
      "took 1.855999231338501 seconds\n",
      "loss: 12.29\tbatch num: 23000/35074\n",
      "took 1.845000982284546 seconds\n",
      "loss: 10.44\tbatch num: 24000/35074\n",
      "took 1.8440001010894775 seconds\n",
      "loss: 14.70\tbatch num: 25000/35074\n",
      "took 1.8450002670288086 seconds\n",
      "loss: 9.01\tbatch num: 26000/35074\n",
      "took 1.8539998531341553 seconds\n",
      "loss: 14.28\tbatch num: 27000/35074\n",
      "took 1.866999626159668 seconds\n",
      "loss: 12.04\tbatch num: 28000/35074\n",
      "took 1.8490016460418701 seconds\n",
      "loss: 9.66\tbatch num: 29000/35074\n",
      "took 1.9809982776641846 seconds\n",
      "loss: 10.31\tbatch num: 30000/35074\n",
      "took 1.8520007133483887 seconds\n",
      "loss: 8.14\tbatch num: 31000/35074\n",
      "took 1.8719995021820068 seconds\n",
      "loss: 12.70\tbatch num: 32000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "loss: 13.17\tbatch num: 33000/35074\n",
      "took 1.8600006103515625 seconds\n",
      "loss: 12.00\tbatch num: 34000/35074\n",
      "took 1.8469994068145752 seconds\n",
      "loss: 12.81\tbatch num: 35000/35074\n",
      "took 1.8419992923736572 seconds\n",
      "Epoch 2 took 65.23599982261658 seconds\n",
      "Average Loss: 30.94902664422989\n",
      "Starting epoch 3\n",
      "loss: 14.05\tbatch num: 0/35074\n",
      "took 0.037999868392944336 seconds\n",
      "loss: 12.03\tbatch num: 1000/35074\n",
      "took 1.869999885559082 seconds\n",
      "loss: 11.52\tbatch num: 2000/35074\n",
      "took 1.8970000743865967 seconds\n",
      "loss: 10.13\tbatch num: 3000/35074\n",
      "took 1.863999843597412 seconds\n",
      "loss: 13.50\tbatch num: 4000/35074\n",
      "took 1.8440005779266357 seconds\n",
      "loss: 11.07\tbatch num: 5000/35074\n",
      "took 1.8429996967315674 seconds\n",
      "loss: 13.69\tbatch num: 6000/35074\n",
      "took 1.874000072479248 seconds\n",
      "loss: 10.35\tbatch num: 7000/35074\n",
      "took 1.8399996757507324 seconds\n",
      "loss: 8.89\tbatch num: 8000/35074\n",
      "took 1.8750009536743164 seconds\n",
      "loss: 14.55\tbatch num: 9000/35074\n",
      "took 1.8679990768432617 seconds\n",
      "loss: 10.85\tbatch num: 10000/35074\n",
      "took 1.8410024642944336 seconds\n",
      "loss: 12.69\tbatch num: 11000/35074\n",
      "took 1.8519978523254395 seconds\n",
      "loss: 11.77\tbatch num: 12000/35074\n",
      "took 1.8429999351501465 seconds\n",
      "loss: 12.73\tbatch num: 13000/35074\n",
      "took 1.845999002456665 seconds\n",
      "loss: 14.60\tbatch num: 14000/35074\n",
      "took 1.8620011806488037 seconds\n",
      "loss: 10.23\tbatch num: 15000/35074\n",
      "took 1.8540000915527344 seconds\n",
      "loss: 10.02\tbatch num: 16000/35074\n",
      "took 1.8399996757507324 seconds\n",
      "loss: 14.26\tbatch num: 17000/35074\n",
      "took 1.8569998741149902 seconds\n",
      "loss: 14.22\tbatch num: 18000/35074\n",
      "took 1.8510003089904785 seconds\n",
      "loss: 12.21\tbatch num: 19000/35074\n",
      "took 1.8619999885559082 seconds\n",
      "loss: 12.65\tbatch num: 20000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "loss: 14.20\tbatch num: 21000/35074\n",
      "took 1.8989996910095215 seconds\n",
      "loss: 14.41\tbatch num: 22000/35074\n",
      "took 1.8530001640319824 seconds\n",
      "loss: 11.71\tbatch num: 23000/35074\n",
      "took 1.8500001430511475 seconds\n",
      "loss: 13.86\tbatch num: 24000/35074\n",
      "took 1.8539996147155762 seconds\n",
      "loss: 16.22\tbatch num: 25000/35074\n",
      "took 1.8610002994537354 seconds\n",
      "loss: 11.20\tbatch num: 26000/35074\n",
      "took 1.8439998626708984 seconds\n",
      "loss: 13.55\tbatch num: 27000/35074\n",
      "took 1.8589997291564941 seconds\n",
      "loss: 14.54\tbatch num: 28000/35074\n",
      "took 1.8530006408691406 seconds\n",
      "loss: 12.23\tbatch num: 29000/35074\n",
      "took 1.8389995098114014 seconds\n",
      "loss: 9.76\tbatch num: 30000/35074\n",
      "took 1.8580005168914795 seconds\n",
      "loss: 12.91\tbatch num: 31000/35074\n",
      "took 1.8569996356964111 seconds\n",
      "loss: 12.22\tbatch num: 32000/35074\n",
      "took 1.8489999771118164 seconds\n",
      "loss: 12.25\tbatch num: 33000/35074\n",
      "took 1.850999116897583 seconds\n",
      "loss: 9.57\tbatch num: 34000/35074\n",
      "took 1.8480021953582764 seconds\n",
      "loss: 13.17\tbatch num: 35000/35074\n",
      "took 1.8649988174438477 seconds\n",
      "Epoch 3 took 65.19900441169739 seconds\n",
      "Average Loss: 30.636548727750778\n",
      "Starting epoch 4\n",
      "loss: 13.28\tbatch num: 0/35074\n",
      "took 0.044997215270996094 seconds\n",
      "loss: 12.81\tbatch num: 1000/35074\n",
      "took 1.873000144958496 seconds\n",
      "loss: 12.50\tbatch num: 2000/35074\n",
      "took 1.8448314666748047 seconds\n",
      "loss: 12.08\tbatch num: 3000/35074\n",
      "took 1.8630011081695557 seconds\n",
      "loss: 11.44\tbatch num: 4000/35074\n",
      "took 1.8439991474151611 seconds\n",
      "loss: 13.06\tbatch num: 5000/35074\n",
      "took 1.850999355316162 seconds\n",
      "loss: 10.24\tbatch num: 6000/35074\n",
      "took 1.8800005912780762 seconds\n",
      "loss: 12.09\tbatch num: 7000/35074\n",
      "took 1.8439991474151611 seconds\n",
      "loss: 8.84\tbatch num: 8000/35074\n",
      "took 1.843000888824463 seconds\n",
      "loss: 11.14\tbatch num: 9000/35074\n",
      "took 1.8589987754821777 seconds\n",
      "loss: 13.61\tbatch num: 10000/35074\n",
      "took 1.8550007343292236 seconds\n",
      "loss: 11.57\tbatch num: 11000/35074\n",
      "took 1.8510003089904785 seconds\n",
      "loss: 9.43\tbatch num: 12000/35074\n",
      "took 1.8489997386932373 seconds\n",
      "loss: 11.08\tbatch num: 13000/35074\n",
      "took 1.8470008373260498 seconds\n",
      "loss: 13.45\tbatch num: 14000/35074\n",
      "took 1.8459992408752441 seconds\n",
      "loss: 7.63\tbatch num: 15000/35074\n",
      "took 1.8630006313323975 seconds\n",
      "loss: 12.88\tbatch num: 16000/35074\n",
      "took 1.8539996147155762 seconds\n",
      "loss: 13.09\tbatch num: 17000/35074\n",
      "took 1.865999698638916 seconds\n",
      "loss: 12.26\tbatch num: 18000/35074\n",
      "took 1.8440005779266357 seconds\n",
      "loss: 9.20\tbatch num: 19000/35074\n",
      "took 1.8549997806549072 seconds\n",
      "loss: 15.15\tbatch num: 20000/35074\n",
      "took 1.8589997291564941 seconds\n",
      "loss: 11.74\tbatch num: 21000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "loss: 13.73\tbatch num: 22000/35074\n",
      "took 1.8720004558563232 seconds\n",
      "loss: 13.29\tbatch num: 23000/35074\n",
      "took 1.8559997081756592 seconds\n",
      "loss: 14.11\tbatch num: 24000/35074\n",
      "took 1.924999713897705 seconds\n",
      "loss: 14.78\tbatch num: 25000/35074\n",
      "took 1.8670003414154053 seconds\n",
      "loss: 11.62\tbatch num: 26000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "loss: 9.84\tbatch num: 27000/35074\n",
      "took 1.8610010147094727 seconds\n",
      "loss: 13.07\tbatch num: 28000/35074\n",
      "took 1.8489990234375 seconds\n",
      "loss: 11.75\tbatch num: 29000/35074\n",
      "took 1.8470005989074707 seconds\n",
      "loss: 13.64\tbatch num: 30000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "loss: 13.75\tbatch num: 31000/35074\n",
      "took 1.8460004329681396 seconds\n",
      "loss: 12.53\tbatch num: 32000/35074\n",
      "took 1.854999303817749 seconds\n",
      "loss: 9.78\tbatch num: 33000/35074\n",
      "took 1.8650000095367432 seconds\n",
      "loss: 13.65\tbatch num: 34000/35074\n",
      "took 1.8459997177124023 seconds\n",
      "loss: 13.01\tbatch num: 35000/35074\n",
      "took 1.8600001335144043 seconds\n",
      "Epoch 4 took 65.26582789421082 seconds\n",
      "Average Loss: 30.2246635556221\n",
      "Starting epoch 5\n",
      "loss: 11.53\tbatch num: 0/35074\n",
      "took 0.050000667572021484 seconds\n",
      "loss: 9.22\tbatch num: 1000/35074\n",
      "took 1.888000249862671 seconds\n",
      "loss: 11.72\tbatch num: 2000/35074\n",
      "took 1.8579998016357422 seconds\n",
      "loss: 14.70\tbatch num: 3000/35074\n",
      "took 1.8780004978179932 seconds\n",
      "loss: 10.13\tbatch num: 4000/35074\n",
      "took 1.8649992942810059 seconds\n",
      "loss: 9.90\tbatch num: 5000/35074\n",
      "took 1.8470003604888916 seconds\n",
      "loss: 13.63\tbatch num: 6000/35074\n",
      "took 1.8649988174438477 seconds\n",
      "loss: 9.83\tbatch num: 7000/35074\n",
      "took 1.8470008373260498 seconds\n",
      "loss: 12.69\tbatch num: 8000/35074\n",
      "took 1.8640003204345703 seconds\n",
      "loss: 8.68\tbatch num: 9000/35074\n",
      "took 1.849998950958252 seconds\n",
      "loss: 8.44\tbatch num: 10000/35074\n",
      "took 1.849001169204712 seconds\n",
      "loss: 11.32\tbatch num: 11000/35074\n",
      "took 1.8470001220703125 seconds\n",
      "loss: 12.71\tbatch num: 12000/35074\n",
      "took 1.8519999980926514 seconds\n",
      "loss: 15.84\tbatch num: 13000/35074\n",
      "took 1.8579998016357422 seconds\n",
      "loss: 10.84\tbatch num: 14000/35074\n",
      "took 1.8540000915527344 seconds\n",
      "loss: 11.61\tbatch num: 15000/35074\n",
      "took 1.853999137878418 seconds\n",
      "loss: 12.18\tbatch num: 16000/35074\n",
      "took 1.8440005779266357 seconds\n",
      "loss: 11.16\tbatch num: 17000/35074\n",
      "took 1.8690006732940674 seconds\n",
      "loss: 12.88\tbatch num: 18000/35074\n",
      "took 1.8619983196258545 seconds\n",
      "loss: 10.93\tbatch num: 19000/35074\n",
      "took 1.8570001125335693 seconds\n",
      "loss: 13.44\tbatch num: 20000/35074\n",
      "took 1.8540008068084717 seconds\n",
      "loss: 15.29\tbatch num: 21000/35074\n",
      "took 1.8459999561309814 seconds\n",
      "loss: 11.36\tbatch num: 22000/35074\n",
      "took 1.8460004329681396 seconds\n",
      "loss: 11.57\tbatch num: 23000/35074\n",
      "took 1.8489997386932373 seconds\n",
      "loss: 11.88\tbatch num: 24000/35074\n",
      "took 1.8510007858276367 seconds\n",
      "loss: 10.61\tbatch num: 25000/35074\n",
      "took 1.8600008487701416 seconds\n",
      "loss: 10.03\tbatch num: 26000/35074\n",
      "took 1.8829987049102783 seconds\n",
      "loss: 12.50\tbatch num: 27000/35074\n",
      "took 1.8449997901916504 seconds\n",
      "loss: 14.37\tbatch num: 28000/35074\n",
      "took 1.8549985885620117 seconds\n",
      "loss: 11.21\tbatch num: 29000/35074\n",
      "took 2.089001417160034 seconds\n",
      "loss: 16.26\tbatch num: 30000/35074\n",
      "took 1.9120523929595947 seconds\n",
      "loss: 14.29\tbatch num: 31000/35074\n",
      "took 1.8819994926452637 seconds\n",
      "loss: 13.47\tbatch num: 32000/35074\n",
      "took 1.8700001239776611 seconds\n",
      "loss: 10.64\tbatch num: 33000/35074\n",
      "took 1.8630003929138184 seconds\n",
      "loss: 12.26\tbatch num: 34000/35074\n",
      "took 1.9319965839385986 seconds\n",
      "loss: 11.33\tbatch num: 35000/35074\n",
      "took 2.012385845184326 seconds\n",
      "Epoch 5 took 65.80143570899963 seconds\n",
      "Average Loss: 30.763203620910645\n",
      "Starting epoch 6\n",
      "loss: 15.59\tbatch num: 0/35074\n",
      "took 0.04101848602294922 seconds\n",
      "loss: 12.75\tbatch num: 1000/35074\n",
      "took 1.9851155281066895 seconds\n",
      "loss: 12.28\tbatch num: 2000/35074\n",
      "took 1.9579260349273682 seconds\n",
      "loss: 13.06\tbatch num: 3000/35074\n",
      "took 1.9611284732818604 seconds\n",
      "loss: 13.38\tbatch num: 4000/35074\n",
      "took 1.8594212532043457 seconds\n",
      "loss: 12.20\tbatch num: 5000/35074\n",
      "took 1.8558425903320312 seconds\n",
      "loss: 10.61\tbatch num: 6000/35074\n",
      "took 1.8499994277954102 seconds\n",
      "loss: 11.41\tbatch num: 7000/35074\n",
      "took 1.8500006198883057 seconds\n",
      "loss: 13.63\tbatch num: 8000/35074\n",
      "took 1.8589997291564941 seconds\n",
      "loss: 11.48\tbatch num: 9000/35074\n",
      "took 1.8630006313323975 seconds\n",
      "loss: 10.32\tbatch num: 10000/35074\n",
      "took 1.8499996662139893 seconds\n",
      "loss: 14.78\tbatch num: 11000/35074\n",
      "took 1.8660001754760742 seconds\n",
      "loss: 12.15\tbatch num: 12000/35074\n",
      "took 1.8829994201660156 seconds\n",
      "loss: 9.88\tbatch num: 13000/35074\n",
      "took 1.8450002670288086 seconds\n",
      "loss: 11.87\tbatch num: 14000/35074\n",
      "took 1.8500008583068848 seconds\n",
      "loss: 18.19\tbatch num: 15000/35074\n",
      "took 1.8589985370635986 seconds\n",
      "loss: 9.83\tbatch num: 16000/35074\n",
      "took 1.8550000190734863 seconds\n",
      "loss: 13.03\tbatch num: 17000/35074\n",
      "took 1.8549995422363281 seconds\n",
      "loss: 14.17\tbatch num: 18000/35074\n",
      "took 1.8599998950958252 seconds\n",
      "loss: 11.02\tbatch num: 19000/35074\n",
      "took 1.8489997386932373 seconds\n",
      "loss: 12.39\tbatch num: 20000/35074\n",
      "took 1.8550004959106445 seconds\n",
      "loss: 10.70\tbatch num: 21000/35074\n",
      "took 1.8599998950958252 seconds\n",
      "loss: 15.81\tbatch num: 22000/35074\n",
      "took 1.876999855041504 seconds\n",
      "loss: 10.35\tbatch num: 23000/35074\n",
      "took 1.8489997386932373 seconds\n",
      "loss: 11.91\tbatch num: 24000/35074\n",
      "took 1.8520030975341797 seconds\n",
      "loss: 13.87\tbatch num: 25000/35074\n",
      "took 1.8569974899291992 seconds\n",
      "loss: 11.44\tbatch num: 26000/35074\n",
      "took 1.8540000915527344 seconds\n",
      "loss: 8.86\tbatch num: 27000/35074\n",
      "took 1.8559999465942383 seconds\n",
      "loss: 9.87\tbatch num: 28000/35074\n",
      "took 1.8589990139007568 seconds\n",
      "loss: 13.59\tbatch num: 29000/35074\n",
      "took 1.909001111984253 seconds\n",
      "loss: 15.76\tbatch num: 30000/35074\n",
      "took 1.8529996871948242 seconds\n",
      "loss: 12.50\tbatch num: 31000/35074\n",
      "took 1.862001657485962 seconds\n",
      "loss: 10.50\tbatch num: 32000/35074\n",
      "took 1.883998155593872 seconds\n",
      "loss: 11.75\tbatch num: 33000/35074\n",
      "took 1.8540000915527344 seconds\n",
      "loss: 15.17\tbatch num: 34000/35074\n",
      "took 1.8620002269744873 seconds\n",
      "loss: 11.44\tbatch num: 35000/35074\n",
      "took 1.8509998321533203 seconds\n",
      "Epoch 6 took 65.63445401191711 seconds\n",
      "Average Loss: 31.397520661354065\n",
      "Starting epoch 7\n",
      "loss: 12.85\tbatch num: 0/35074\n",
      "took 0.0449981689453125 seconds\n",
      "loss: 10.08\tbatch num: 1000/35074\n",
      "took 1.8789997100830078 seconds\n",
      "loss: 13.52\tbatch num: 2000/35074\n",
      "took 1.8530004024505615 seconds\n",
      "loss: 13.36\tbatch num: 3000/35074\n",
      "took 1.8499999046325684 seconds\n",
      "loss: 12.77\tbatch num: 4000/35074\n",
      "took 1.8580000400543213 seconds\n",
      "loss: 8.38\tbatch num: 5000/35074\n",
      "took 1.8510000705718994 seconds\n",
      "loss: 10.19\tbatch num: 6000/35074\n",
      "took 1.8499999046325684 seconds\n",
      "loss: 10.87\tbatch num: 7000/35074\n",
      "took 1.8710002899169922 seconds\n",
      "loss: 11.14\tbatch num: 8000/35074\n",
      "took 1.8510000705718994 seconds\n",
      "loss: 9.70\tbatch num: 9000/35074\n",
      "took 1.8589997291564941 seconds\n",
      "loss: 14.78\tbatch num: 10000/35074\n",
      "took 1.8550012111663818 seconds\n",
      "loss: 11.02\tbatch num: 11000/35074\n",
      "took 1.8529977798461914 seconds\n",
      "loss: 10.96\tbatch num: 12000/35074\n",
      "took 1.8520009517669678 seconds\n",
      "loss: 10.37\tbatch num: 13000/35074\n",
      "took 1.8499996662139893 seconds\n",
      "loss: 13.93\tbatch num: 14000/35074\n",
      "took 1.8610005378723145 seconds\n",
      "loss: 11.65\tbatch num: 15000/35074\n",
      "took 1.8469996452331543 seconds\n",
      "loss: 9.58\tbatch num: 16000/35074\n",
      "took 1.8450005054473877 seconds\n",
      "loss: 11.13\tbatch num: 17000/35074\n",
      "took 1.8600001335144043 seconds\n",
      "loss: 15.00\tbatch num: 18000/35074\n",
      "took 1.8560004234313965 seconds\n",
      "loss: 12.63\tbatch num: 19000/35074\n",
      "took 1.851999282836914 seconds\n",
      "loss: 10.39\tbatch num: 20000/35074\n",
      "took 1.8499996662139893 seconds\n",
      "loss: 12.91\tbatch num: 21000/35074\n",
      "took 1.8949999809265137 seconds\n",
      "loss: 12.39\tbatch num: 22000/35074\n",
      "took 1.8440003395080566 seconds\n",
      "loss: 15.62\tbatch num: 23000/35074\n",
      "took 1.8730003833770752 seconds\n",
      "loss: 9.08\tbatch num: 24000/35074\n",
      "took 1.8609619140625 seconds\n",
      "loss: 11.78\tbatch num: 25000/35074\n",
      "took 1.8440005779266357 seconds\n",
      "loss: 9.04\tbatch num: 26000/35074\n",
      "took 1.867999792098999 seconds\n",
      "loss: 11.69\tbatch num: 27000/35074\n",
      "took 1.8480002880096436 seconds\n",
      "loss: 7.68\tbatch num: 28000/35074\n",
      "took 1.8490002155303955 seconds\n",
      "loss: 12.44\tbatch num: 29000/35074\n",
      "took 1.861999750137329 seconds\n",
      "loss: 12.89\tbatch num: 30000/35074\n",
      "took 1.866999864578247 seconds\n",
      "loss: 9.85\tbatch num: 31000/35074\n",
      "took 1.8550000190734863 seconds\n",
      "loss: 14.12\tbatch num: 32000/35074\n",
      "took 1.8429999351501465 seconds\n",
      "loss: 13.21\tbatch num: 33000/35074\n",
      "took 1.8420000076293945 seconds\n",
      "loss: 14.22\tbatch num: 34000/35074\n",
      "took 1.884000539779663 seconds\n",
      "loss: 15.41\tbatch num: 35000/35074\n",
      "took 1.865999460220337 seconds\n",
      "Epoch 7 took 65.23096227645874 seconds\n",
      "Average Loss: 31.224685072898865\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "model = TestModel().cuda()\n",
    "loss_fn = torch.nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, model, loss_fn)\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"testmodel.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d8ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is within five pins 30.7% of the time\n",
      "Model is within ten pins 50.8% of the time\n",
      "Model is within fifteen pins 67.5% of the time\n"
     ]
    }
   ],
   "source": [
    "fivePins = 0\n",
    "tenPins = 0\n",
    "fifteenPins = 0\n",
    "total = 0\n",
    "for x in range(len(testData)):\n",
    "    total += 1\n",
    "    pred = model(testData[x][0])\n",
    "    diff = abs(pred - testData[x][1])\n",
    "    if diff <= 5:\n",
    "        fivePins += 1\n",
    "    if diff <= 10:\n",
    "        tenPins += 1\n",
    "    if diff <= 15:\n",
    "        fifteenPins += 1\n",
    "print(f\"Model is within five pins {fivePins * 100 / total}% of the time\")\n",
    "print(f\"Model is within ten pins {tenPins * 100 / total}% of the time\")\n",
    "print(f\"Model is within fifteen pins {fifteenPins * 100 / total}% of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27eecb-7708-47d7-b8c1-fe6f22dc543e",
   "metadata": {},
   "source": [
    "The R-squared for most of these models hovers around 0.73, which is pretty good :)\n",
    "\n",
    "\n",
    "Model with L1 Loss, adam at lr=0.0005, batchsize = 64, and 8 epochs:\n",
    "    Model is within five pins 30.5% of the time\n",
    "    Model is within ten pins 50.4% of the time\n",
    "    Model is within fifteen pins 65.8% of the time\n",
    "\n",
    "Model with L1 Loss, adam at lr=0.001, batchsize = 64, and 8 epochs:\n",
    "    Model is within five pins 30.2% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 66.7% of the time\n",
    "\n",
    "More neurons Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 29.5% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.9% of the time\n",
    "\n",
    "less neurons Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 30.6% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 67.5% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 31.8% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with MSE Loss, adam at lr=0.001, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 29.1% of the time\n",
    "    Model is within ten pins 49.8% of the time\n",
    "    Model is within fifteen pins 66.5% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, adam at lr=0.00005, batchsize = 32, and 8 epochs:\n",
    "    Model is within five pins 31.8% of the time\n",
    "    Model is within ten pins 51.2% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.0001, batchsize = 32, and 8 epochs:\\\n",
    "    Model is within five pins 30.0% of the time\n",
    "    Model is within ten pins 51.9% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 16, and 8 epochs:\\\n",
    "    Model is within five pins 29.5% of the tim e\n",
    "    Model is within ten pins 50.6% of the time\n",
    "    Model is within fifteen pins 67.8% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 4, and 8 epochs:\n",
    "    Model is within five pins 31.3% of the time\n",
    "    Model is within ten pins 52.2% of the time\n",
    "    Model is within fifteen pins 68.2% of the time\n",
    "\n",
    "Leaky Relu Model with L1 Loss, RMSprop at lr=0.000001, batchsize = 1, and 8 epochs:\n",
    "    Model is within five pins 31.0% of the time\n",
    "    Model is within ten pins 52.9% of the time\n",
    "    Model is within fifteen pins 67.7% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb320c-bab8-4409-b4b2-4e5014ef2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, calculate mean score. just gonna manually do this by iterating over dataset\n",
    "\"\"\"\n",
    "tSum = 0\n",
    "tCount = 0\n",
    "for row in trainData:\n",
    "    tCount += 1\n",
    "    tSum += int(row[1])\n",
    "    if not tCount % 100000:\n",
    "        print(f\"finished with {tCount}\")\n",
    "mean = tSum/tCount\n",
    "print(mean)\n",
    "\"\"\"\n",
    "mean=191.9994600624403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914576fc-83a6-41ce-80e7-964c6b812a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SST = 0\n",
    "tCount = 0\n",
    "for row in trainData:\n",
    "    tCount += 1\n",
    "    SST += (int(row[1]) - mean) ** 2\n",
    "    if not tCount % 50000:\n",
    "        print(tCount)\n",
    "print(SST)\n",
    "\"\"\"\n",
    "SST = 1108731521.6719563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824181c4-cc27-4fdc-888c-3a8fc4a2e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "tensor([2.9814e+08], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#calculate SSE, which sums of squared residuals (errors)\n",
    "SSE = 0\n",
    "tCount = 0\n",
    "for row in trainData:\n",
    "    tCount += 1\n",
    "    pred = model(row[0])\n",
    "    resid = pred - row[1]\n",
    "    SSE += resid ** 2\n",
    "    if not tCount % 50000:\n",
    "        print(tCount)\n",
    "print(SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8289a485-2f35-4782-9012-d3f61b5a1580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311], device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(1 - (SSE/SST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e57e7-035f-4487-91c9-454679154d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
