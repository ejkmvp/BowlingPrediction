{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83abf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4533b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49651224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# dataset that directly loads the file into memory and then retrieves data as needed\\n# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\\nclass BowlingDataset(Dataset):\\n    def __init__(self, fileName):\\n        self.f = open(fileName, \"rb\")\\n        self.length = int(os.stat(fileName).st_size/27)\\n        #if self.length > 1500000:\\n        #    self.length = 1500000\\n        self.f.seek(0)\\n        self.fileData = self.f.read()\\n        self.f.close()\\n    def __len__(self):\\n        return self.length\\n    \\n    def __getitem__(self, idx):\\n        gameData = self.fileData[idx * 27: idx * 27 + 27]\\n        tempArray = []\\n        for x in range(25):\\n            tempArray += [v for v in format(gameData[x], \"08b\")]\\n        finalScore = gameData[-1] * 256 + gameData[-2]\\n        inputArray = [float(v) for v in tempArray][:120]\\n        output = finalScore\\n        return torch.tensor(inputArray[:]), torch.tensor(float(output))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedc9665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\\nclass BowlingDataset(Dataset):\\n    def __init__(self, fileName):\\n        self.f = open(fileName, \"rb\")\\n        self.length = int(os.stat(fileName).st_size/27)\\n        \\n    def __len__(self):\\n        return self.length\\n    \\n    def __getitem__(self, idx):\\n        self.f.seek(idx * 27)\\n        gameData = self.f.read(27)\\n        tempArray = []\\n        for x in range(25):\\n            tempArray += [v for v in format(gameData[x], \"08b\")]\\n        finalScore = gameData[-1] * 256 + gameData[-2]\\n        inputArray = [float(v) for v in tempArray][:120]\\n        output = finalScore\\n        return torch.tensor(inputArray[:]), torch.tensor(float(output))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset that pulls data from file as requested\n",
    "\"\"\" This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.f.seek(idx * 27)\n",
    "        gameData = self.f.read(27)\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446fe4d5-830a-4a7e-872e-5bcd450b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that loads all data into GPU memory\n",
    "#this works significantly better but we are hard limited by vram. maybe try to look into memory pinning more\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        if self.length > 1500000:\n",
    "            self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        # build entire array into memory\n",
    "        self.inputArray=[]\n",
    "        self.finalScoreArray=[]\n",
    "        for v in range(self.length):\n",
    "            gameData = self.f.read(27)\n",
    "            tempArray = []\n",
    "            for x in range(25):\n",
    "                tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "            self.finalScoreArray.append(gameData[-1] * 256 + gameData[-2])\n",
    "            self.inputArray.append([float(v) for v in tempArray][:120])\n",
    "            if not v % 10000:\n",
    "                print(f\"loaded {v} our of {self.length}\")\n",
    "        self.inputTensor = torch.tensor(self.inputArray).to(\"cuda\")\n",
    "        self.outputTensor = torch.tensor(self.finalScoreArray).to(\"cuda\")\n",
    "        self.inputArray = []\n",
    "        self.finalScoreArray = []\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputTensor[idx], self.outputTensor[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16999c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.relu_stack(x)\n",
    "        output = torch.nn.Sigmoid()(logits)\n",
    "        return output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    totalBatches = len(dataloader)\n",
    "    model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction + loss\n",
    "        prediction = model(x).squeeze(1)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            print(f\"loss: {loss.item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "            print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    model.eval() # need to look into what this does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    print(f\"Average Loss: {test_loss/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0211bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed9adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 0 our of 1122352\n",
      "loaded 10000 our of 1122352\n",
      "loaded 20000 our of 1122352\n",
      "loaded 30000 our of 1122352\n",
      "loaded 40000 our of 1122352\n",
      "loaded 50000 our of 1122352\n",
      "loaded 60000 our of 1122352\n",
      "loaded 70000 our of 1122352\n",
      "loaded 80000 our of 1122352\n",
      "loaded 90000 our of 1122352\n",
      "loaded 100000 our of 1122352\n",
      "loaded 110000 our of 1122352\n",
      "loaded 120000 our of 1122352\n",
      "loaded 130000 our of 1122352\n",
      "loaded 140000 our of 1122352\n",
      "loaded 150000 our of 1122352\n",
      "loaded 160000 our of 1122352\n",
      "loaded 170000 our of 1122352\n",
      "loaded 180000 our of 1122352\n",
      "loaded 190000 our of 1122352\n",
      "loaded 200000 our of 1122352\n",
      "loaded 210000 our of 1122352\n",
      "loaded 220000 our of 1122352\n",
      "loaded 230000 our of 1122352\n",
      "loaded 240000 our of 1122352\n",
      "loaded 250000 our of 1122352\n",
      "loaded 260000 our of 1122352\n",
      "loaded 270000 our of 1122352\n",
      "loaded 280000 our of 1122352\n",
      "loaded 290000 our of 1122352\n",
      "loaded 300000 our of 1122352\n",
      "loaded 310000 our of 1122352\n",
      "loaded 320000 our of 1122352\n",
      "loaded 330000 our of 1122352\n",
      "loaded 340000 our of 1122352\n",
      "loaded 350000 our of 1122352\n",
      "loaded 360000 our of 1122352\n",
      "loaded 370000 our of 1122352\n",
      "loaded 380000 our of 1122352\n",
      "loaded 390000 our of 1122352\n",
      "loaded 400000 our of 1122352\n",
      "loaded 410000 our of 1122352\n",
      "loaded 420000 our of 1122352\n",
      "loaded 430000 our of 1122352\n",
      "loaded 440000 our of 1122352\n",
      "loaded 450000 our of 1122352\n",
      "loaded 460000 our of 1122352\n",
      "loaded 470000 our of 1122352\n",
      "loaded 480000 our of 1122352\n",
      "loaded 490000 our of 1122352\n",
      "loaded 500000 our of 1122352\n",
      "loaded 510000 our of 1122352\n",
      "loaded 520000 our of 1122352\n",
      "loaded 530000 our of 1122352\n",
      "loaded 540000 our of 1122352\n",
      "loaded 550000 our of 1122352\n",
      "loaded 560000 our of 1122352\n",
      "loaded 570000 our of 1122352\n",
      "loaded 580000 our of 1122352\n",
      "loaded 590000 our of 1122352\n",
      "loaded 600000 our of 1122352\n",
      "loaded 610000 our of 1122352\n",
      "loaded 620000 our of 1122352\n",
      "loaded 630000 our of 1122352\n",
      "loaded 640000 our of 1122352\n",
      "loaded 650000 our of 1122352\n",
      "loaded 660000 our of 1122352\n",
      "loaded 670000 our of 1122352\n",
      "loaded 680000 our of 1122352\n",
      "loaded 690000 our of 1122352\n",
      "loaded 700000 our of 1122352\n",
      "loaded 710000 our of 1122352\n",
      "loaded 720000 our of 1122352\n",
      "loaded 730000 our of 1122352\n",
      "loaded 740000 our of 1122352\n",
      "loaded 750000 our of 1122352\n",
      "loaded 760000 our of 1122352\n",
      "loaded 770000 our of 1122352\n",
      "loaded 780000 our of 1122352\n",
      "loaded 790000 our of 1122352\n",
      "loaded 800000 our of 1122352\n",
      "loaded 810000 our of 1122352\n",
      "loaded 820000 our of 1122352\n",
      "loaded 830000 our of 1122352\n",
      "loaded 840000 our of 1122352\n",
      "loaded 850000 our of 1122352\n",
      "loaded 860000 our of 1122352\n",
      "loaded 870000 our of 1122352\n",
      "loaded 880000 our of 1122352\n",
      "loaded 890000 our of 1122352\n",
      "loaded 900000 our of 1122352\n",
      "loaded 910000 our of 1122352\n",
      "loaded 920000 our of 1122352\n",
      "loaded 930000 our of 1122352\n",
      "loaded 940000 our of 1122352\n",
      "loaded 950000 our of 1122352\n",
      "loaded 960000 our of 1122352\n",
      "loaded 970000 our of 1122352\n",
      "loaded 980000 our of 1122352\n",
      "loaded 990000 our of 1122352\n",
      "loaded 1000000 our of 1122352\n",
      "loaded 1010000 our of 1122352\n",
      "loaded 1020000 our of 1122352\n",
      "loaded 1030000 our of 1122352\n",
      "loaded 1040000 our of 1122352\n",
      "loaded 1050000 our of 1122352\n",
      "loaded 1060000 our of 1122352\n",
      "loaded 1070000 our of 1122352\n",
      "loaded 1080000 our of 1122352\n",
      "loaded 1090000 our of 1122352\n",
      "loaded 1100000 our of 1122352\n",
      "loaded 1110000 our of 1122352\n",
      "loaded 1120000 our of 1122352\n",
      "loaded 0 our of 1000\n"
     ]
    }
   ],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74655ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "17537\n"
     ]
    }
   ],
   "source": [
    "# test to see if pinning is working\n",
    "for batch_ndx, sample in enumerate(trainDataLoader):\n",
    "    if batch_ndx > 5:\n",
    "        break\n",
    "    print(sample[1].is_pinned())\n",
    "    print(sample[1].is_cuda)\n",
    "print(len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8191c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "loss: 48.64\tbatch num: 0/17537\n",
      "took 0.59993577003479 seconds\n",
      "loss: 14.01\tbatch num: 1000/17537\n",
      "took 2.447175979614258 seconds\n",
      "loss: 11.64\tbatch num: 2000/17537\n",
      "took 2.3979651927948 seconds\n",
      "loss: 13.19\tbatch num: 3000/17537\n",
      "took 2.369196653366089 seconds\n",
      "loss: 16.04\tbatch num: 4000/17537\n",
      "took 2.392025947570801 seconds\n",
      "loss: 14.06\tbatch num: 5000/17537\n",
      "took 2.3783626556396484 seconds\n",
      "loss: 14.09\tbatch num: 6000/17537\n",
      "took 2.3726351261138916 seconds\n",
      "loss: 12.03\tbatch num: 7000/17537\n",
      "took 2.376999616622925 seconds\n",
      "loss: 15.85\tbatch num: 8000/17537\n",
      "took 2.3690481185913086 seconds\n",
      "loss: 11.86\tbatch num: 9000/17537\n",
      "took 2.3749992847442627 seconds\n",
      "loss: 11.48\tbatch num: 10000/17537\n",
      "took 2.36403751373291 seconds\n",
      "loss: 11.45\tbatch num: 11000/17537\n",
      "took 2.422710418701172 seconds\n",
      "loss: 10.78\tbatch num: 12000/17537\n",
      "took 2.3701069355010986 seconds\n",
      "loss: 12.17\tbatch num: 13000/17537\n",
      "took 2.382520914077759 seconds\n",
      "loss: 11.46\tbatch num: 14000/17537\n",
      "took 2.361063003540039 seconds\n",
      "loss: 13.49\tbatch num: 15000/17537\n",
      "took 2.368999719619751 seconds\n",
      "loss: 11.99\tbatch num: 16000/17537\n",
      "took 2.4200210571289062 seconds\n",
      "loss: 11.06\tbatch num: 17000/17537\n",
      "took 2.3871777057647705 seconds\n",
      "Epoch 0 took 42.470980644226074 seconds\n",
      "Average Loss: 30.610272645950317\n",
      "Starting epoch 1\n",
      "loss: 12.85\tbatch num: 0/17537\n",
      "took 0.04300189018249512 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "X:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14.01\tbatch num: 1000/17537\n",
      "took 2.3937435150146484 seconds\n",
      "loss: 11.91\tbatch num: 2000/17537\n",
      "took 2.378053665161133 seconds\n",
      "loss: 13.75\tbatch num: 3000/17537\n",
      "took 2.3750267028808594 seconds\n",
      "loss: 12.15\tbatch num: 4000/17537\n",
      "took 2.3909482955932617 seconds\n",
      "loss: 12.05\tbatch num: 5000/17537\n",
      "took 2.3746511936187744 seconds\n",
      "loss: 12.04\tbatch num: 6000/17537\n",
      "took 2.395317554473877 seconds\n",
      "loss: 14.12\tbatch num: 7000/17537\n",
      "took 2.3790385723114014 seconds\n",
      "loss: 12.91\tbatch num: 8000/17537\n",
      "took 2.3886942863464355 seconds\n",
      "loss: 13.54\tbatch num: 9000/17537\n",
      "took 2.3561174869537354 seconds\n",
      "loss: 14.78\tbatch num: 10000/17537\n",
      "took 2.363999366760254 seconds\n",
      "loss: 12.96\tbatch num: 11000/17537\n",
      "took 2.374544143676758 seconds\n",
      "loss: 11.31\tbatch num: 12000/17537\n",
      "took 2.388523817062378 seconds\n",
      "loss: 12.94\tbatch num: 13000/17537\n",
      "took 2.3710875511169434 seconds\n",
      "loss: 15.53\tbatch num: 14000/17537\n",
      "took 2.379133701324463 seconds\n",
      "loss: 12.35\tbatch num: 15000/17537\n",
      "took 2.3739523887634277 seconds\n",
      "loss: 10.55\tbatch num: 16000/17537\n",
      "took 2.369908094406128 seconds\n",
      "loss: 11.23\tbatch num: 17000/17537\n",
      "took 2.3759982585906982 seconds\n",
      "Epoch 1 took 41.78757286071777 seconds\n",
      "Average Loss: 31.26742112636566\n",
      "Starting epoch 2\n",
      "loss: 14.70\tbatch num: 0/17537\n",
      "took 0.034996747970581055 seconds\n",
      "loss: 12.77\tbatch num: 1000/17537\n",
      "took 2.385009527206421 seconds\n",
      "loss: 13.81\tbatch num: 2000/17537\n",
      "took 2.3760409355163574 seconds\n",
      "loss: 13.99\tbatch num: 3000/17537\n",
      "took 2.384633779525757 seconds\n",
      "loss: 14.41\tbatch num: 4000/17537\n",
      "took 2.372002124786377 seconds\n",
      "loss: 13.37\tbatch num: 5000/17537\n",
      "took 2.3829421997070312 seconds\n",
      "loss: 10.85\tbatch num: 6000/17537\n",
      "took 2.3837242126464844 seconds\n",
      "loss: 13.65\tbatch num: 7000/17537\n",
      "took 2.386019706726074 seconds\n",
      "loss: 11.29\tbatch num: 8000/17537\n",
      "took 2.3701493740081787 seconds\n",
      "loss: 12.10\tbatch num: 9000/17537\n",
      "took 2.3661086559295654 seconds\n",
      "loss: 13.36\tbatch num: 10000/17537\n",
      "took 2.361828327178955 seconds\n",
      "loss: 10.53\tbatch num: 11000/17537\n",
      "took 2.3840208053588867 seconds\n",
      "loss: 11.88\tbatch num: 12000/17537\n",
      "took 2.376386880874634 seconds\n",
      "loss: 11.94\tbatch num: 13000/17537\n",
      "took 2.360710859298706 seconds\n",
      "loss: 11.87\tbatch num: 14000/17537\n",
      "took 2.371110200881958 seconds\n",
      "loss: 10.79\tbatch num: 15000/17537\n",
      "took 2.372002363204956 seconds\n",
      "loss: 13.04\tbatch num: 16000/17537\n",
      "took 2.388380289077759 seconds\n",
      "loss: 10.40\tbatch num: 17000/17537\n",
      "took 2.370309591293335 seconds\n",
      "Epoch 2 took 41.74940586090088 seconds\n",
      "Average Loss: 30.75036644935608\n",
      "Starting epoch 3\n",
      "loss: 10.29\tbatch num: 0/17537\n",
      "took 0.03599882125854492 seconds\n",
      "loss: 15.01\tbatch num: 1000/17537\n",
      "took 2.386751174926758 seconds\n",
      "loss: 13.09\tbatch num: 2000/17537\n",
      "took 2.359687566757202 seconds\n",
      "loss: 12.17\tbatch num: 3000/17537\n",
      "took 2.381216526031494 seconds\n",
      "loss: 12.74\tbatch num: 4000/17537\n",
      "took 2.4360668659210205 seconds\n",
      "loss: 13.81\tbatch num: 5000/17537\n",
      "took 2.499999523162842 seconds\n",
      "loss: 12.34\tbatch num: 6000/17537\n",
      "took 2.3859684467315674 seconds\n",
      "loss: 13.21\tbatch num: 7000/17537\n",
      "took 2.395911931991577 seconds\n",
      "loss: 12.72\tbatch num: 8000/17537\n",
      "took 2.365063428878784 seconds\n",
      "loss: 11.22\tbatch num: 9000/17537\n",
      "took 2.454031229019165 seconds\n",
      "loss: 12.69\tbatch num: 10000/17537\n",
      "took 2.4423463344573975 seconds\n",
      "loss: 14.35\tbatch num: 11000/17537\n",
      "took 2.4646780490875244 seconds\n",
      "loss: 12.27\tbatch num: 12000/17537\n",
      "took 2.4363009929656982 seconds\n",
      "loss: 15.89\tbatch num: 13000/17537\n",
      "took 2.4629974365234375 seconds\n",
      "loss: 14.15\tbatch num: 14000/17537\n",
      "took 2.456000328063965 seconds\n",
      "loss: 14.68\tbatch num: 15000/17537\n",
      "took 2.453566789627075 seconds\n",
      "loss: 11.67\tbatch num: 16000/17537\n",
      "took 2.3620269298553467 seconds\n",
      "loss: 12.98\tbatch num: 17000/17537\n",
      "took 2.376026153564453 seconds\n",
      "Epoch 3 took 42.48126173019409 seconds\n",
      "Average Loss: 31.484785437583923\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "model = TestModel().cuda()\n",
    "loss_fn = torch.nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, model)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"testmodel.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8ef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
