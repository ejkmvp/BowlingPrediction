{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49651224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that pulls data from file as requested\n",
    "\"\"\" This dataset is very memory efficient, but it is heavily limited by storage bandwidth (i think)\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.f.seek(idx * 27)\n",
    "        gameData = self.f.read(27)\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fe4d5-830a-4a7e-872e-5bcd450b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that loads all data into GPU memory\n",
    "\"\"\" #this works significantly better but we are hard limited by vram. maybe try to look into memory pinning more\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        if self.length > 1500000:\n",
    "            self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        # build entire array into memory\n",
    "        self.inputArray=[]\n",
    "        self.finalScoreArray=[]\n",
    "        for v in range(self.length):\n",
    "            gameData = self.f.read(27)\n",
    "            tempArray = []\n",
    "            for x in range(25):\n",
    "                tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "            self.finalScoreArray.append(gameData[-1] * 256 + gameData[-2])\n",
    "            self.inputArray.append([float(v) for v in tempArray][:120])\n",
    "            if not v % 10000:\n",
    "                print(f\"loaded {v} our of {self.length}\")\n",
    "        self.inputTensor = torch.tensor(self.inputArray).to(\"cuda\")\n",
    "        self.outputTensor = torch.tensor(self.finalScoreArray).to(\"cuda\")\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputTensor[idx], self.outputTensor[idx]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16999c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.relu_stack(x)\n",
    "        output = torch.nn.Sigmoid()(logits)\n",
    "        return output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    totalBatches = len(dataloader)\n",
    "    model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction + loss\n",
    "        prediction = model(x.cuda()).squeeze(1)\n",
    "        loss = loss_fn(prediction, y.cuda())\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            print(f\"loss: {loss.item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "            print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    model.eval() # need to look into what this does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x.cuda())\n",
    "            test_loss += loss_fn(pred, y.cuda()).item()\n",
    "    \n",
    "    print(f\"Average Loss: {test_loss/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True, pin_memory=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48a178-f1e4-4c4d-9a3b-3a174f61980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that games loaded from the traindataset are valid\n",
    "badGameCounter = 0\n",
    "for x in range(len(trainData)):\n",
    "    game, fScore = trainData[x]\n",
    "    for frameNum in range(5):\n",
    "        for pinNum in range(10):\n",
    "            if game[20 * frameNum + pinNum] == float(0):\n",
    "                if(game[20 * frameNum + pinNum + 10] == float(1)):\n",
    "                    badGameCounter += 1\n",
    "                    print(f\"problem with game number {x}\")\n",
    "                    print(20 * frameNum + pinNum)\n",
    "                    print(20 * frameNum + pinNum + 10)\n",
    "                    print(game)\n",
    "                    assert(False)\n",
    "    if not x % 1000:\n",
    "        print(f\"finish {x} games\")\n",
    "        print(f\"{badGameCounter} bad games found \")\n",
    "print(badGameCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61d6a3-544d-4b36-b1fb-e4e541bb20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[11][0][95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74655ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test to see if pinning is working\n",
    "for batch_ndx, sample in enumerate(trainDataLoader):\n",
    "    if batch_ndx > 5:\n",
    "        break\n",
    "    print(sample[1].is_pinned())\n",
    "    print(sample[1].is_cuda)\n",
    "print(len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel().cuda()\n",
    "loss_fn = torch.nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, model)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"testmodel.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8ef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
