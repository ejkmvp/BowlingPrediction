{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a550f7fb-d2fe-4560-afe2-d75f90e2c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs a bit faster, but not a lot\n",
    "# it may just be best to load the whole model onto the gpu \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e78616-7497-4974-99d1-aaf25481723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd449d3-cc80-45d8-b37d-c934c5f5df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that directly loads the file into memory and then retrieves data as needed\n",
    "# this helps deal with the file read bottlenecks, but the data has to be transformed and then loaded to the gpu\n",
    "class BowlingDataset(Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.f = open(fileName, \"rb\")\n",
    "        self.length = int(os.stat(fileName).st_size/27)\n",
    "        #if self.length > 1500000:\n",
    "        #    self.length = 1500000\n",
    "        self.f.seek(0)\n",
    "        self.fileData = self.f.read()\n",
    "        self.f.close()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gameData = self.fileData[idx * 27: idx * 27 + 27]\n",
    "        tempArray = []\n",
    "        for x in range(25):\n",
    "            tempArray += [v for v in format(gameData[x], \"08b\")]\n",
    "        finalScore = gameData[-1] * 256 + gameData[-2]\n",
    "        inputArray = [float(v) for v in tempArray][:120]\n",
    "        output = finalScore\n",
    "        return torch.tensor(inputArray[:]), torch.tensor(float(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160b508a-55fb-4a31-97ea-8219deab8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.logits = self.relu_stack(x)\n",
    "        self.output = torch.nn.Sigmoid()(self.logits)\n",
    "        return self.output * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966c3fbc-7c63-40a9-a7af-af56dd5e2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, modelList, lossFnList, optimizerList):\n",
    "    size = len(dataloader.dataset) # get number of samples\n",
    "    numModels = len(modelList)\n",
    "    totalBatches = len(dataloader)\n",
    "    predictionList = [0] * len(modelList)\n",
    "    lossList = [0] * len(modelList)\n",
    "    for model in modelList:\n",
    "        model.train() # need to look into what this exactly does\n",
    "    startTime = time.time()\n",
    "    for batchNum, (x, y) in enumerate(dataloader):\n",
    "        #grab model input and label as tensors on the gpu\n",
    "        xTensor = x.cuda()\n",
    "        yTensor = y.cuda()\n",
    "        \n",
    "        # the point of all these consecutive for loops is that the cuda operations should be async\n",
    "        # and then automatically synced up when the result tensor is needed\n",
    "        \n",
    "        # zero out gradients of each optimizer\n",
    "        for x in range(numModels):\n",
    "            optimizerList[x].zero_grad()\n",
    "            \n",
    "        # get predictions\n",
    "        for x in range(numModels):\n",
    "            predictionList[x] = modelList[x](xTensor)\n",
    "        \n",
    "        #compute losses\n",
    "        for x in range(numModels):\n",
    "            lossList[x] = lossFnList[x](predictionList[x].squeeze(1), yTensor)\n",
    "        \n",
    "        #run backprogagation\n",
    "        for x in range(numModels):\n",
    "            lossList[x].backward()\n",
    "        \n",
    "        #update weights\n",
    "        for x in range(numModels):\n",
    "            optimizerList[x].step()\n",
    "        \n",
    "        if not batchNum % 1000:\n",
    "            for x in range(numModels):\n",
    "                print(f\"Model{x} -- loss: {lossList[x].item():.2f}\\tbatch num: {batchNum}/{totalBatches}\")\n",
    "                print(f\"took {time.time() - startTime} seconds\")\n",
    "            startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073b2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, modelList, loss_fn):\n",
    "    for model in modelList:\n",
    "        model.eval() # need to look into what this exactly does\n",
    "    size = len(dataloader.dataset)\n",
    "    numBatches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    total_losses = [0] * len(modelList)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            xTensor = x.cuda()\n",
    "            yTensor = y.cuda()\n",
    "            for x in range(len(modelList)):\n",
    "                pred = model(xTensor)\n",
    "                total_losses[x] += loss_fn(pred, yTensor).item()\n",
    "    for x in range(len(modelList)):\n",
    "        print(f\"model{x} Test Set -- Average Loss: {total_losses[x]/numBatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d772f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ccf50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = BowlingDataset(\"ScoreDetailDataset.txt\")\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True, pin_memory=True) #pin memory doesnt do shit bc the memory has to be grabbed from a physical file\n",
    "testData = BowlingDataset(\"ScoreDetailDatasetVSplit.txt\")\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9f567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Model0 -- loss: 3355.13\tbatch num: 0/62321\n",
      "took 0.3431823253631592 seconds\n",
      "Model1 -- loss: 3826.80\tbatch num: 0/62321\n",
      "took 0.3431823253631592 seconds\n",
      "Model2 -- loss: 4478.77\tbatch num: 0/62321\n",
      "took 0.3431823253631592 seconds\n",
      "Model3 -- loss: 4639.71\tbatch num: 0/62321\n",
      "took 0.3431823253631592 seconds\n",
      "Model0 -- loss: 227.69\tbatch num: 1000/62321\n",
      "took 38.52545619010925 seconds\n",
      "Model1 -- loss: 230.31\tbatch num: 1000/62321\n",
      "took 38.526453495025635 seconds\n",
      "Model2 -- loss: 258.45\tbatch num: 1000/62321\n",
      "took 38.526453495025635 seconds\n",
      "Model3 -- loss: 34462.34\tbatch num: 1000/62321\n",
      "took 38.526453495025635 seconds\n",
      "Model0 -- loss: 180.89\tbatch num: 2000/62321\n",
      "took 39.35813307762146 seconds\n",
      "Model1 -- loss: 200.59\tbatch num: 2000/62321\n",
      "took 39.35908794403076 seconds\n",
      "Model2 -- loss: 227.10\tbatch num: 2000/62321\n",
      "took 39.35908794403076 seconds\n",
      "Model3 -- loss: 36746.45\tbatch num: 2000/62321\n",
      "took 39.35908794403076 seconds\n",
      "Model0 -- loss: 247.10\tbatch num: 3000/62321\n",
      "took 27.760605335235596 seconds\n",
      "Model1 -- loss: 267.66\tbatch num: 3000/62321\n",
      "took 27.760605335235596 seconds\n",
      "Model2 -- loss: 289.52\tbatch num: 3000/62321\n",
      "took 27.760605335235596 seconds\n",
      "Model3 -- loss: 32985.84\tbatch num: 3000/62321\n",
      "took 27.760605335235596 seconds\n",
      "Model0 -- loss: 210.68\tbatch num: 4000/62321\n",
      "took 31.983027696609497 seconds\n",
      "Model1 -- loss: 206.41\tbatch num: 4000/62321\n",
      "took 31.983027696609497 seconds\n",
      "Model2 -- loss: 191.93\tbatch num: 4000/62321\n",
      "took 31.984017848968506 seconds\n",
      "Model3 -- loss: 33401.31\tbatch num: 4000/62321\n",
      "took 31.984017848968506 seconds\n",
      "Model0 -- loss: 197.29\tbatch num: 5000/62321\n",
      "took 29.47174882888794 seconds\n",
      "Model1 -- loss: 206.00\tbatch num: 5000/62321\n",
      "took 29.47274923324585 seconds\n",
      "Model2 -- loss: 224.19\tbatch num: 5000/62321\n",
      "took 29.47274923324585 seconds\n",
      "Model3 -- loss: 32910.27\tbatch num: 5000/62321\n",
      "took 29.47274923324585 seconds\n",
      "Model0 -- loss: 207.91\tbatch num: 6000/62321\n",
      "took 29.016464948654175 seconds\n",
      "Model1 -- loss: 218.64\tbatch num: 6000/62321\n",
      "took 29.016464948654175 seconds\n",
      "Model2 -- loss: 234.47\tbatch num: 6000/62321\n",
      "took 29.016464948654175 seconds\n",
      "Model3 -- loss: 37667.95\tbatch num: 6000/62321\n",
      "took 29.016464948654175 seconds\n",
      "Model0 -- loss: 220.15\tbatch num: 7000/62321\n",
      "took 32.03570604324341 seconds\n",
      "Model1 -- loss: 225.97\tbatch num: 7000/62321\n",
      "took 32.03570604324341 seconds\n",
      "Model2 -- loss: 229.39\tbatch num: 7000/62321\n",
      "took 32.03570604324341 seconds\n",
      "Model3 -- loss: 35448.34\tbatch num: 7000/62321\n",
      "took 32.03570604324341 seconds\n",
      "Model0 -- loss: 308.76\tbatch num: 8000/62321\n",
      "took 30.322514295578003 seconds\n",
      "Model1 -- loss: 302.38\tbatch num: 8000/62321\n",
      "took 30.322514295578003 seconds\n",
      "Model2 -- loss: 296.21\tbatch num: 8000/62321\n",
      "took 30.323545932769775 seconds\n",
      "Model3 -- loss: 34573.95\tbatch num: 8000/62321\n",
      "took 30.323545932769775 seconds\n",
      "Model0 -- loss: 235.19\tbatch num: 9000/62321\n",
      "took 30.4062762260437 seconds\n",
      "Model1 -- loss: 240.77\tbatch num: 9000/62321\n",
      "took 30.4062762260437 seconds\n",
      "Model2 -- loss: 231.86\tbatch num: 9000/62321\n",
      "took 30.4062762260437 seconds\n",
      "Model3 -- loss: 37393.11\tbatch num: 9000/62321\n",
      "took 30.4062762260437 seconds\n",
      "Model0 -- loss: 234.10\tbatch num: 10000/62321\n",
      "took 23.497631788253784 seconds\n",
      "Model1 -- loss: 229.58\tbatch num: 10000/62321\n",
      "took 23.498631477355957 seconds\n",
      "Model2 -- loss: 249.41\tbatch num: 10000/62321\n",
      "took 23.498631477355957 seconds\n",
      "Model3 -- loss: 36419.88\tbatch num: 10000/62321\n",
      "took 23.498631477355957 seconds\n",
      "Model0 -- loss: 195.39\tbatch num: 11000/62321\n",
      "took 27.727179765701294 seconds\n",
      "Model1 -- loss: 200.26\tbatch num: 11000/62321\n",
      "took 27.727179765701294 seconds\n",
      "Model2 -- loss: 214.65\tbatch num: 11000/62321\n",
      "took 27.727179765701294 seconds\n",
      "Model3 -- loss: 37400.70\tbatch num: 11000/62321\n",
      "took 27.727179765701294 seconds\n",
      "Model0 -- loss: 235.65\tbatch num: 12000/62321\n",
      "took 29.339280605316162 seconds\n",
      "Model1 -- loss: 222.80\tbatch num: 12000/62321\n",
      "took 29.339280605316162 seconds\n",
      "Model2 -- loss: 237.05\tbatch num: 12000/62321\n",
      "took 29.340267419815063 seconds\n",
      "Model3 -- loss: 36976.86\tbatch num: 12000/62321\n",
      "took 29.340267419815063 seconds\n",
      "Model0 -- loss: 246.63\tbatch num: 13000/62321\n",
      "took 28.930274486541748 seconds\n",
      "Model1 -- loss: 253.86\tbatch num: 13000/62321\n",
      "took 28.930274486541748 seconds\n",
      "Model2 -- loss: 249.21\tbatch num: 13000/62321\n",
      "took 28.930274486541748 seconds\n",
      "Model3 -- loss: 36100.06\tbatch num: 13000/62321\n",
      "took 28.930274486541748 seconds\n",
      "Model0 -- loss: 295.48\tbatch num: 14000/62321\n",
      "took 29.72846531867981 seconds\n",
      "Model1 -- loss: 289.01\tbatch num: 14000/62321\n",
      "took 29.72846531867981 seconds\n",
      "Model2 -- loss: 300.09\tbatch num: 14000/62321\n",
      "took 29.72946572303772 seconds\n",
      "Model3 -- loss: 35378.55\tbatch num: 14000/62321\n",
      "took 29.72946572303772 seconds\n",
      "Model0 -- loss: 252.57\tbatch num: 15000/62321\n",
      "took 26.187180757522583 seconds\n",
      "Model1 -- loss: 251.03\tbatch num: 15000/62321\n",
      "took 26.187180757522583 seconds\n",
      "Model2 -- loss: 256.43\tbatch num: 15000/62321\n",
      "took 26.187180757522583 seconds\n",
      "Model3 -- loss: 36597.58\tbatch num: 15000/62321\n",
      "took 26.187180757522583 seconds\n",
      "Model0 -- loss: 338.93\tbatch num: 16000/62321\n",
      "took 30.586223602294922 seconds\n",
      "Model1 -- loss: 341.68\tbatch num: 16000/62321\n",
      "took 30.586223602294922 seconds\n",
      "Model2 -- loss: 355.06\tbatch num: 16000/62321\n",
      "took 30.586223602294922 seconds\n",
      "Model3 -- loss: 33845.83\tbatch num: 16000/62321\n",
      "took 30.586223602294922 seconds\n",
      "Model0 -- loss: 115.52\tbatch num: 17000/62321\n",
      "took 29.133595943450928 seconds\n",
      "Model1 -- loss: 113.64\tbatch num: 17000/62321\n",
      "took 29.133595943450928 seconds\n",
      "Model2 -- loss: 121.66\tbatch num: 17000/62321\n",
      "took 29.133595943450928 seconds\n",
      "Model3 -- loss: 35676.45\tbatch num: 17000/62321\n",
      "took 29.133595943450928 seconds\n",
      "Model0 -- loss: 164.83\tbatch num: 18000/62321\n",
      "took 32.56950616836548 seconds\n",
      "Model1 -- loss: 163.50\tbatch num: 18000/62321\n",
      "took 32.57046413421631 seconds\n",
      "Model2 -- loss: 154.36\tbatch num: 18000/62321\n",
      "took 32.57046413421631 seconds\n",
      "Model3 -- loss: 37085.73\tbatch num: 18000/62321\n",
      "took 32.57046413421631 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDataLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfnList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimList\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstartTime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m test_loop(testDataLoader, modelList, lossfnList[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, modelList, lossFnList, optimizerList)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#update weights\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numModels):\n\u001b[1;32m---> 33\u001b[0m     \u001b[43moptimizerList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batchNum \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numModels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\adam.py:395\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    393\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43mstep_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[0;32m    398\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model0 = TestModel().cuda()\n",
    "optim0 = torch.optim.Adam(model0.parameters(), lr=0.001)\n",
    "lossfn0 = torch.nn.MSELoss().cuda()\n",
    "\n",
    "model1 = TestModel().cuda()\n",
    "optim1 = torch.optim.Adam(model1.parameters(), lr=0.0005)\n",
    "lossfn1 = torch.nn.MSELoss().cuda()\n",
    "\n",
    "model2 = TestModel().cuda()\n",
    "optim2 = torch.optim.Adam(model2.parameters(), lr=0.0001)\n",
    "lossfn2 = torch.nn.MSELoss().cuda()\n",
    "\n",
    "model3 = TestModel().cuda()\n",
    "optim3 = torch.optim.SGD(model3.parameters(), lr=0.001)\n",
    "lossfn3 = torch.nn.MSELoss().cuda()\n",
    "\n",
    "modelList = [model0, model1, model2, model3]\n",
    "optimList = [optim0, optim1, optim2, optim3]\n",
    "lossfnList = [lossfn0, lossfn1, lossfn2, lossfn3]\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    startTime = time.time()\n",
    "    print(f\"Starting epoch {t}\")\n",
    "    train_loop(trainDataLoader, modelList, lossfnList, optimList)\n",
    "    print(f\"Epoch {t} took {time.time() - startTime} seconds\")\n",
    "    test_loop(testDataLoader, modelList, lossfnList[0])\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4049ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
